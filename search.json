[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cognitive Models",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#why-julia",
    "href": "index.html#why-julia",
    "title": "Cognitive Models",
    "section": "Why Julia?",
    "text": "Why Julia?\nJulia - the new cool kid on the scientific block - is a modern programming language with many benefits when compared with R or Python. Importantly, it is currently the only language in which we can fit all the cognitive models under a Bayesian framework using a unified interface like Turing and SequentialSamplingModels.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#why-bayesian",
    "href": "index.html#why-bayesian",
    "title": "Cognitive Models",
    "section": "Why Bayesian?",
    "text": "Why Bayesian?\nUnfortunately, cognitive models often involve distributions for which Frequentist estimations are not yet implemented, and usually contain a lot of parameters (due to the presence of random effects), which makes traditional algorithms fail to converge. Simply put, the Bayesian approach is the only one currently robust enough to fit these somewhat models.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#looking-for-coauthors",
    "href": "index.html#looking-for-coauthors",
    "title": "Cognitive Models",
    "section": "Looking for Coauthors",
    "text": "Looking for Coauthors\nAs this is a fast-evolving field (both from the theoretical - with new models being proposed - and the technical side - with improvements to the packages and the algorithms), the book needs to be future-resilient and updatable by contributors to keep up with the latest best practices.\nThis project can only be achieved by a team, and I suspect no single person has currently all the skills and knowledge to cover all the content. We need many people who have strengths in various aspects, such as Julia/Turing, theory, writing, making plots etc. Most importantly, this project can serve as a way for us to learn more about this approach to psychological science.\nIf you are interested in the project, you can let us know by opening an issue or getting in touch.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "1_introduction.html",
    "href": "1_introduction.html",
    "title": "1  Bayesian Modeling in Julia",
    "section": "",
    "text": "1.1 Brief Intro to Julia and Turing\nGoal is to teach just enough so that the reader understands the code. We won’t be discussing things like plotting (as it highly depends on the package used).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bayesian Modeling in Julia</span>"
    ]
  },
  {
    "objectID": "1_introduction.html#brief-intro-to-julia-and-turing",
    "href": "1_introduction.html#brief-intro-to-julia-and-turing",
    "title": "1  Bayesian Modeling in Julia",
    "section": "",
    "text": "To go further\n\n\n\n\nModern Julia Workflows: Julia tutorial that takes you from zero to hero.\n\n\n\n\n1.1.1 Installing Julia and Packages\nTODO.\n\n\n1.1.2 Julia Basics\n\n\n\n\n\n\nNotable Differences with Python and R\n\n\n\nThese are the most common sources of confusion and errors for newcomers to Julia:\n\n1-indexing: Similarly to R, Julia uses 1-based indexing, which means that the first element of a vector is x[1] (not x[0] as in Python).\nPositional; Keyword arguments: Julia functions makes a clear distinction between positional and keyword arguments, and both are often separated by ;. Positional arguments are typically passed without a name, while keyword arguments must be named (e.g., scatter(0, 0; color=:red)). Some functions might look like somefunction(; arg1=val1, arg2=val2).\nSymbols: Some arguments are prefixed with : (e.g., :red in scatter(0, 0; color=:red)). These symbols are like character strings that are not manipulable (there are more efficient).\nExplicit vectorization: Julia does not vectorize operations by default. You need to use a dot . in front of functions and operators to have it apply element by element. For example, sin.([0, 1, 2]) will apply the sin() function to each element of its vector.\nIn-place operations: Julia has a strong emphasis on performance, and in-place operations are often used to avoid unnecessary memory allocations. When functions modify their input “in-place” (without returns), a band ! is used. For example, assuming x = [0] (1-element vector containing 0), push!(x, 2) will modify x in place (it is equivalent to x = push(x, 2)).\nMacros: Some functions start with @. These are called macros and are used to manipulate the code before it is run. For example, @time will measure the time it takes to run the code that follows.\nUnicode: Julia is a modern language to supports unicode characters, which are used a lot for mathematical operations. You can get the mu μ character by typing \\mu and pressing TAB.\n\n\n\n\n\n1.1.3 Generate Data from Normal Distribution\n\nusing Turing, Distributions, Random\nusing Makie\n\n# Random sample from a Normal(μ=100, σ=15)\niq = rand(Normal(100, 15), 500)\n\n\n\nCode\nfig = Figure()\nax = Axis(fig[1, 1], title=\"Distribution\")\ndensity!(ax, iq)\nfig\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie C:\\Users\\domma\\.julia\\packages\\Makie\\VRavR\\src\\scenes.jl:220\n\n\n\n\n\n\n\n\n\n\n\n1.1.4 Recover Distribution Parameters with Turing\n\n@model function model_gaussian(x)\n    # Priors\n    μ ~ Uniform(0, 200)\n    σ ~ Uniform(0, 30)\n\n    # Check against each datapoint\n    for i in 1:length(x)\n        x[i] ~ Normal(μ, σ)\n    end\nend\n\nfit_gaussian = model_gaussian(iq)\nchain_gaussian = sample(fit_gaussian, NUTS(), 400)\n\nInspecting the chain variable will show various posterior statistics (including the mean, standard deviation, and diagnostic indices).\n\nchain_gaussian\n\n\nChains MCMC chain (400×14×1 Array{Float64, 3}):\nIterations        = 201:1:600\nNumber of chains  = 1\nSamples per chain = 400\nWall duration     = 8.8 seconds\nCompute duration  = 8.8 seconds\nparameters        = μ, σ\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\nSummary Statistics\n  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   e ⋯\n      Symbol   Float64   Float64   Float64    Float64    Float64   Float64     ⋯\n           μ   99.2403    0.6727    0.0333   414.3604   324.9996    0.9993     ⋯\n           σ   14.4973    0.4440    0.0187   561.5709   284.5407    0.9976     ⋯\n                                                                1 column omitted\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%      97.5% \n      Symbol   Float64   Float64   Float64   Float64    Float64 \n           μ   97.9096   98.7663   99.2552   99.7769   100.4228\n           σ   13.6853   14.1811   14.5066   14.7917    15.3761\n\n\n\n\nFor the purpose of this book, we will mostly focus on the 95% Credible Interval (CI), and we will assume that a parameter is “significant” if its CI does not include 0.\n\n# Summary (95% CI)\nhpd(chain_gaussian)\n\n\nHPD\n  parameters     lower      upper \n      Symbol   Float64    Float64 \n           μ   97.8594   100.3178\n           σ   13.5687    15.2885",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bayesian Modeling in Julia</span>"
    ]
  },
  {
    "objectID": "1_introduction.html#bayesian-linear-models",
    "href": "1_introduction.html#bayesian-linear-models",
    "title": "1  Bayesian Modeling in Julia",
    "section": "1.2 Bayesian Linear Models",
    "text": "1.2 Bayesian Linear Models\n\nUnderstand what the parameters mean (intercept, slopes, sigma): needs a nice graph (animation?) to illustrate that. Simple linear regression in Turing. Introduce Bayesian estimation and priors over parameters",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bayesian Modeling in Julia</span>"
    ]
  },
  {
    "objectID": "1_introduction.html#hierarchical-models",
    "href": "1_introduction.html#hierarchical-models",
    "title": "1  Bayesian Modeling in Julia",
    "section": "1.3 Hierarchical Models",
    "text": "1.3 Hierarchical Models\n\nSimpson’s paradox, random effects\nThese models can be leveraged to obtain individual indices useful to study interindividual differences (see last chapter).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bayesian Modeling in Julia</span>"
    ]
  },
  {
    "objectID": "2_predictors.html",
    "href": "2_predictors.html",
    "title": "2  Predictors",
    "section": "",
    "text": "2.1 Categorical predictors (Condition, Group, …)\nIn the previous chapter, we have mainly focused on the relationship between a response variable and a single continuous predictor.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Predictors</span>"
    ]
  },
  {
    "objectID": "2_predictors.html#categorical-predictors-condition-group",
    "href": "2_predictors.html#categorical-predictors-condition-group",
    "title": "2  Predictors",
    "section": "",
    "text": "Contrasts, …",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Predictors</span>"
    ]
  },
  {
    "objectID": "2_predictors.html#interactions",
    "href": "2_predictors.html#interactions",
    "title": "2  Predictors",
    "section": "2.2 Interactions",
    "text": "2.2 Interactions\n\nTodo.\n\nNested interactions (difference between R’s formula fac1 * fac2 and fac1 / fac2 and how to specify that in Julia/Turing)\nUse of the @formula macro to create the design matrix.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Predictors</span>"
    ]
  },
  {
    "objectID": "2_predictors.html#ordered-predictors-likert-scales",
    "href": "2_predictors.html#ordered-predictors-likert-scales",
    "title": "2  Predictors",
    "section": "2.3 Ordered predictors (Likert Scales)",
    "text": "2.3 Ordered predictors (Likert Scales)\nLikert scales, i.e., ordered multiple discrete choices are often used in surveys and questionnaires. While such data is often treated as a continuous variable, such assumption is not necessarily valid. Indeed, distance between the choices is not necessarily equal. For example, the difference between “strongly agree” and “agree” might not be the same as between “agree” and “neutral”. Even when using integers like 1, 2, 3, 4; people might implicitly process “4” as more extreme relative to “3” as “3” to “2”.\n\n\nThe probabilities assigned to discrete probability descriptors are not necessarily equidistant (https://github.com/zonination/perceptions)\n\nWhat can we do to better reflect the cognitive process underlying a Likert scale responses? Monotonic effects.\n\nHow to do Monotonic effects in Turing?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Predictors</span>"
    ]
  },
  {
    "objectID": "2_predictors.html#non-linear-relationships",
    "href": "2_predictors.html#non-linear-relationships",
    "title": "2  Predictors",
    "section": "2.4 Non-linear Relationships",
    "text": "2.4 Non-linear Relationships\n\n\nusing Downloads, CSV, DataFrames, Random\nusing Turing, Distributions\nusing GLMakie\n\nRandom.seed!(123)  # For reproducibility\n\ndf = CSV.read(Downloads.download(\"https://raw.githubusercontent.com/DominiqueMakowski/CognitiveModels/main/data/nonlinear.csv\"), DataFrame)\n\n# Show 10 first rows\nscatter(df.Age, df.SexualDrive, color=:black)\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie C:\\Users\\domma\\.julia\\packages\\Makie\\VRavR\\src\\scenes.jl:220\n\n\n\n\n\n\n2.4.1 Polynomials\nRaw vs. orthogonal polynomials.\n\n\n2.4.2 Generalized Additive Models (GAMs)\nTodo.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Predictors</span>"
    ]
  },
  {
    "objectID": "3a_scales.html",
    "href": "3a_scales.html",
    "title": "3  Bounded Variables",
    "section": "",
    "text": "3.1 The Problem with Linear Models\nLet’s take the data from Makowski et al. (2023) that contains data from participants that underwent the Mini-IPIP6 personality test and the PID-5 BF questionnaire for “maladaptive” personality. We will focus on the “Disinhibition” trait from the PID-5 BF questionnaire. Note that altough it is usually computed as the average of items a 4-point Likert scales [0-3], this study used analog slides to obtain more finer-grained scores.\nusing Downloads, CSV, DataFrames, Random\nusing Turing, Distributions, StatsFuns\nusing GLMakie\n\nRandom.seed!(123)  # For reproducibility\n\ndf = CSV.read(Downloads.download(\"https://raw.githubusercontent.com/DominiqueMakowski/CognitiveModels/main/data/makowski2023.csv\"), DataFrame)\n\n# Show 10 first rows\nfirst(df, 10)\n\n# Plot the distribution of \"Disinhibition\"\nhist(df.Disinhibition, normalization = :pdf, color=:darkred, bins=40)\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie C:\\Users\\domma\\.julia\\packages\\Makie\\VRavR\\src\\scenes.jl:220\nWe will then fit a simple Gaussian model (an “intercept-only” linear model) that estimates the mean and the standard deviation of our variable of interest.\n@model function model_Gaussian(x)\n\n    # Priors\n    σ ~ truncated(Normal(0, 1); lower=0)  # Strictly positive half normal distribution\n    μ ~ Normal(0, 3)\n\n    # Iterate through every observation\n    for i in 1:length(x)\n        # Likelihood family\n        x[i] ~ Normal(μ, σ)\n    end\nend\n\n# Fit the model with the data\nfit_Gaussian = model_Gaussian(df.Disinhibition)\n# Sample results using MCMC\nchain_Gaussian = sample(fit_Gaussian, NUTS(), 400)\nLet see if the model managed to recover the mean and standard deviation of the data:\nCode\nprintln(\"Mean of the data: $(round(mean(df.Disinhibition); digits=3)) vs. mean from the model: $(round(mean(chain_Gaussian[:μ]); digits=3))\")\nprintln(\"SD of the data: $(round(std(df.Disinhibition); digits=3)) vs. SD from the model: $(round(mean(chain_Gaussian[:σ]); digits=3))\")\n\n\nMean of the data: 1.064 vs. mean from the model: 1.066\nSD of the data: 0.616 vs. SD from the model: 0.618\nImpressive! The model managed to almost perfectly recover the mean and standard deviation of the data. That means we must have a good model, right? Not so fast!\nLinear models are by definition designed at recovering the mean of the outcome variables (and its SD, assuming it is inavriant across groups). That does not mean that they can capture the full complexity of the data.\nLet us then jump straight into generating predictions from the model and plotting the results against the actual data to see how well the model fits the data (a procedure called the posterior predictive check).\nCode\npred = predict(model_Gaussian([(missing) for i in 1:length(df.Disinhibition)]), chain_Gaussian)\npred = Array(pred)\nCode\nfig = hist(df.Disinhibition, normalization = :pdf, color=:darkred, bins=40)\nfor i in 1:length(chain_Gaussian)\n    lines!(Makie.KernelDensity.kde(pred[i, :]), alpha=0.1, color=:black)\nend\nfig\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie C:\\Users\\domma\\.julia\\packages\\Makie\\VRavR\\src\\scenes.jl:220\nAs we can see, the model assumes that the data is normally distributed, in a way that allows for negative values and values above 3, which are not possible because our variable is, by design, bounded between 0 and 3 (at it is the result of the mean of variables between 0 and 3). The linear model might thus not be the best choice for this type of data.",
    "crumbs": [
      "Scales and Choices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bounded Variables</span>"
    ]
  },
  {
    "objectID": "3a_scales.html#rescaling",
    "href": "3a_scales.html#rescaling",
    "title": "3  Bounded Variables",
    "section": "3.2 Rescaling",
    "text": "3.2 Rescaling\nContinuous variables can be trivially rescaled, which is often done to improve the interpretability of the results. For instance, a z-score is a rescaled variable with a mean of 0 and a standard deviation of 1. Importantly, rescaling variables does not change the variable’s distribution or the absolute relationship between variables. It does not alter the fundamental conclusions from an analysis, but can help with the interpretation of the results (or computational performance).\nTwo common rescalings are standardization (mean=0, SD=1) and normalization (range 0-1). The benefits of standardization are the interpretation in terms of deviation (which can be compared across variables). The benefits of normalization are the interpretation in terms of “proportion” (percentage): a value of \\(0.5\\) (i.e., \\(50\\%\\)) means that the value is in the middle of the range. The latter is particularly useful for bounded variables, as it redefines their bounds to 0 and 1 and allows for a more intuitive interpretation (e.g., “Condition B led to an increase of 20% of the rating on that scale”).\nLet’s rescale the “Disinhibition” variable to a 0-1 range:\n\nfunction data_rescale(x; old_range=[minimum(x), maximum(x)], new_range=[0, 1])\n    return (x .- old_range[1]) ./ (old_range[2] - old_range[1]) .* (new_range[2] - new_range[1]) .+ new_range[1]\nend\n\n# Rescale the variable\ndf.Disinhibition2 = data_rescale(df.Disinhibition; old_range=[0, 3], new_range=[0, 1])\n\n\n\nCode\n# Visualize\nfig = hist(df.Disinhibition2, normalization = :pdf, color=:darkred, bins=40)\nxlims!(-0.1, 1.1)\nfig\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie C:\\Users\\domma\\.julia\\packages\\Makie\\VRavR\\src\\scenes.jl:220",
    "crumbs": [
      "Scales and Choices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bounded Variables</span>"
    ]
  },
  {
    "objectID": "3a_scales.html#modified-beta-distribution",
    "href": "3a_scales.html#modified-beta-distribution",
    "title": "3  Bounded Variables",
    "section": "3.3 Modified Beta Distribution",
    "text": "3.3 Modified Beta Distribution\nOne good potential alternative to linear models for bounded variables is to use a Beta distribution instead of a Gaussian distribution, as the Beta distribution is bounded between 0 and 1 (not including them). Moreover, Beta distributions are powerful and can model a wide range of shapes, including normal-like distributions, but also uniformly spread data and data clustered at one or both ends.\nThe Beta distribution is typically defined by two parameters, alpha \\(\\alpha\\) and beta \\(\\beta\\), which are the shape parameters. Unfortunately, these parameters are not very intuitive, and so we often use a “reparametrization” of the Beta distribution to define it by its mean mu \\(\\mu\\) and “precision” phi \\(\\phi\\) (referring to the narrowness of the distribution). This is particularly convenient in the context of regressions, as these parameters are more interpretable and can be directly linked to the predictors.\nHere is the code to redefine a Beta distribution based on the mean mu \\(\\mu\\) and precision phi \\(\\phi\\), that converts them to the shape parameters alpha \\(\\alpha\\) and beta \\(\\beta\\).\n\nfunction BetaMuPhi(μ, ϕ)\n    return Beta(μ * ϕ, (1 - μ) * ϕ)\nend\n\n\n\n\n\n\n\nCaution\n\n\n\nTODO: Replace if it is supported somewhere (e.g., Distributions.jl)\n\n\nNote that for this modified Beta distribution, the mean mu \\(\\mu\\) and precision phi \\(\\phi\\) can impact the distribution in suprising ways.",
    "crumbs": [
      "Scales and Choices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bounded Variables</span>"
    ]
  },
  {
    "objectID": "3a_scales.html#beta-models",
    "href": "3a_scales.html#beta-models",
    "title": "3  Bounded Variables",
    "section": "3.4 Beta Models",
    "text": "3.4 Beta Models\nNote that we have a suited distribution for our bounded variable, we can now fit a Beta model to the rescaled variable. However, there is one important issue to address: the Beta distribution is not defined at exactly 0 and 1, and we currently rescaled our variable to be between 0 and 1, possibly including them.\nOne common trick is to actually rescale our variable to be within \\([0, 1]\\) by nudging the zeros and ones to be just above and below, respectively. For this, one can use the function eps(), which returns the smallest possible number. For instance, one can rescale the variable to be in the range [eps(), 1 - eps()], equivalent to \\([0.000...1, 0.999...]\\).\n\ndf.Disinhibition3 = data_rescale(df.Disinhibition; old_range=[0, 3], new_range=[eps(), 1 - eps()])\n\nFor the priors, we will use a Beta distribution \\(Beta(1.25, 1.25)\\) for mu \\(\\mu\\) that is naturally bounded at \\(]0, 1[\\), peaks at 0.5, and assign less plausibility to extreme values. A Gamma distribution \\(Gamma(1.5, 15)\\) for phi \\(\\phi\\) is a good choice for the precision, as it is naturally bounded at \\(]0, +\\infty[\\).\n\n\nCode\nfig = Figure()\nax1 = Axis(fig[1, 1], xlabel=\"Value of μ\", ylabel=\"Plausibility\", title=\"Prior for μ ~ Beta(1.25, 1.25)\", yticksvisible=false, yticklabelsvisible=false,)\nband!(ax1, range(0, 1, length=1000), 0, pdf.(Beta(1.25, 1.25), range(0, 1, length=1000)), color=:red)\nylims!(0, 1.25)\nax1 = Axis(fig[1, 2], xlabel=\"Value of ϕ\", ylabel=\"Plausibility\", title=\"Prior for ϕ ~ Gamma(1.5, 15)\", yticksvisible=false, yticklabelsvisible=false,)\nband!(ax1, range(0, 120, length=1000), 0, pdf.(Gamma(1.5, 15), range(0, 120, length=1000)), color=:blue)\n# ylims!(0, 0.06)\nfig\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie C:\\Users\\domma\\.julia\\packages\\Makie\\VRavR\\src\\scenes.jl:220\n\n\n\n\n\nWe can now fit a Beta model to the rescaled variable.\n\n@model function model_Beta(x)\n    μ ~ Beta(1.25, 1.25)\n    ϕ ~ Gamma(1.5, 15)\n    \n    for i in 1:length(x)\n        x[i] ~ BetaMuPhi(μ, ϕ)\n    end\nend\n\nfit_Beta = model_Beta(df.Disinhibition3)\nchain_Beta = sample(fit_Beta, NUTS(), 500)\n\n\n\n\n\n\n\nNote\n\n\n\nNote that it is also common to express mu \\(\\mu\\) on the logit scale. In other words, the prior on mu \\(\\mu\\) can be specified using any unbounded distributions (e.g., \\(Normal(0, 1)\\)), which is convenient to set effect coefficients. The resulting value is passed through a logistic function that transforms any values to the [0, 1] range (suited for mu \\(\\mu\\)). We will demonstrate this below.\n\n\nLet us see make a posterior predictive check to see how well the model fits the data.\n\n\nCode\npred = predict(model_Beta([(missing) for i in 1:nrow(df)]), chain_Beta)\npred = Array(pred)\n\nfig = hist(df.Disinhibition3, normalization = :pdf, color=:darkred, bins=40)\nfor i in 1:length(chain_Beta)\n    density!(pred[i, :], color=(:black, 0), strokecolor = (:black, 0.1), strokewidth = 3, boundary=(0, 1))\nend\nfig\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie C:\\Users\\domma\\.julia\\packages\\Makie\\VRavR\\src\\scenes.jl:220\n\n\n\n\n\nIt is… quite terrible. Why?",
    "crumbs": [
      "Scales and Choices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bounded Variables</span>"
    ]
  },
  {
    "objectID": "3a_scales.html#excluding-extreme-observations",
    "href": "3a_scales.html#excluding-extreme-observations",
    "title": "3  Bounded Variables",
    "section": "3.5 Excluding Extreme Observations",
    "text": "3.5 Excluding Extreme Observations\nOne of the main issues is that, as you can se from the histogram, there is a high number of observations clumped at zero. This creates a bimodal distribution which makes standard unimodal distributions fail to capture the data (note this issues is not addressed by linear models, which estimates will get biased by this configuration away from the actual “mean” of the variable).\nOne simple, although not ideal, solution is to exclude extreme values (zeros or ones). Beyond the statistical sanitization benefits, one could argue that these “floor” and “ceiling” effects might correspond to a different cognitive process (this will be important in the later part of this chapter).\n\n\n\n\n\n\nNote\n\n\n\nFor instance, in the case of a bounded scale type of trials, participants might actually use a dual strategy in order to lower the cognitive load. For each item, they would judge 1) whether their answer is “completely” yes or no (i.e., 0 or 1) and 2) if not, they would then engage in a more nuanced and costly evaluation of the degree (i.e., use continuous values in between).\n\n\nLet’s create a new variable without the extreme values.\n\n# Filter out extreme values\nvar_noextreme = df.Disinhibition2[(df.Disinhibition2 .&gt; 0) .& (df.Disinhibition2 .&lt; 1)]\n\nThis time, we will add another trick to make the model more robust (note that this is a general improvement that we are introducing here but that is not related to the current issue at hand of the extreme values). The current parameter mu \\(\\mu\\) is defined on the \\(]0, 1[\\) range. Although this is not an issue in our model where we don’t have any predictors, these types of bounded parameters can be a bit problematic in the context of regressions, where the effect of predictors can push the parameter outside of its bounds. For example, imagine that the algorithm pics a value of \\(0.45\\) for mu \\(\\mu\\) from the prior, and then picks a value of \\(+0.30\\) for the effect of a potential predictor (e.g., an experimental condition). This would result in a value of \\(0.75\\), which is outside the range of possible, and would make the model fail to converge.\nOne common solution (at the heard of the so-called logistic models) is to express mu \\(\\mu\\) on the logit scale using the logistic function (available in the StatsFuns package). The logistic function is a simple transformation that maps any value (from the unbounded range \\(]-\\infty, +\\infty[\\))) to the 0, 1 range. We can now specify the prior for mu \\(\\mu\\) on the logit scale as a normal distribution (e.g., \\(Normal(0, 3)\\)) without worrying about the estimates going out of bounds.\n\n\nCode\nxaxis = range(-10, 10, length=1000)\n\nfig = Figure()\nax1 = Axis(fig[1:2, 1], xlabel=\"Value of μ on the logit scale\", ylabel=\"Actual value of μ\", title=\"Logistic function\")\nlines!(ax1, xaxis, logistic.(xaxis), color=:red, linewidth=2)\nax2 = Axis(fig[1, 2], xlabel=\"Value of μ on the logit scale\", ylabel=\"Plausibility\", title=\"Prior for μ ~ Normal(0, 3)\", yticksvisible=false, yticklabelsvisible=false,)\nlines!(ax2, xaxis, pdf.(Normal(0, 3), xaxis), color=:blue, linewidth=2)\nax3 = Axis(fig[2, 2], xlabel=\"Value of μ after logistic transformation\", ylabel=\"Plausibility\", yticksvisible=false, yticklabelsvisible=false,)\nlines!(ax3, logistic.(xaxis), pdf.(Normal(0, 3), xaxis), color=:green, linewidth=2)\nfig\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie C:\\Users\\domma\\.julia\\packages\\Makie\\VRavR\\src\\scenes.jl:220\n\n\n\n\n\n\n@model function model_Beta(x)\n    μ ~ Normal(0, 3)\n    ϕ ~ Gamma(1.5, 15)\n    \n    for i in 1:length(x)\n        norm_μ = logistic(μ)  # Normalize (0-1) the μ parameter\n        x[i] ~ BetaMuPhi(norm_μ, ϕ)\n    end\nend\n\n# Refit\nfit_Beta = model_Beta(var_noextreme)\nchain_Beta = sample(fit_Beta, NUTS(), 500)\n\nLet us now make a posterior predictive check to see how well the model fits the data.\n\n\nCode\npred = predict(model_Beta([(missing) for i in 1:length(var_noextreme)]), chain_Beta)\npred = Array(pred)\n\nfig = hist(var_noextreme, normalization = :pdf, color=:darkred, bins=40)\nfor i in 1:length(chain_Beta)\n    density!(pred[i, :], color=(:black, 0), strokecolor = (:black, 0.1), strokewidth = 3, boundary=(0, 1))\nend\nfig\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie C:\\Users\\domma\\.julia\\packages\\Makie\\VRavR\\src\\scenes.jl:220\n\n\n\n\n\nRemoving the extreme values improved the fit. However, it is not a perfect solution.",
    "crumbs": [
      "Scales and Choices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bounded Variables</span>"
    ]
  },
  {
    "objectID": "3a_scales.html#ordered-beta-models",
    "href": "3a_scales.html#ordered-beta-models",
    "title": "3  Bounded Variables",
    "section": "3.6 Ordered Beta Models",
    "text": "3.6 Ordered Beta Models\nNeed help to implement this in Turing!\n\nhttps://cran.r-project.org/web/packages/ordbetareg/vignettes/package_introduction.html\nhttps://stats.andrewheiss.com/compassionate-clam/notebook/ordbeta.html#ordered-beta-regression\nhttps://www.robertkubinec.com/post/limited_dvs/\nKubinec (2022)",
    "crumbs": [
      "Scales and Choices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bounded Variables</span>"
    ]
  },
  {
    "objectID": "3a_scales.html#mixed-ordered-beta-models",
    "href": "3a_scales.html#mixed-ordered-beta-models",
    "title": "3  Bounded Variables",
    "section": "3.7 Mixed Ordered Beta Models",
    "text": "3.7 Mixed Ordered Beta Models\nComplex example walkthrough. Demonstrate how to add random effects to the model.\n\n\n\n\nMakowski, Dominique, An Shu Te, Stephanie Kirk, Ngoi Zi Liang, and SH Annabel Chen. 2023. “A Novel Visual Illusion Paradigm Provides Evidence for a General Factor of Illusion Sensitivity and Personality Correlates.” Scientific Reports 13 (1): 6594.",
    "crumbs": [
      "Scales and Choices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bounded Variables</span>"
    ]
  },
  {
    "objectID": "3b_choices.html",
    "href": "3b_choices.html",
    "title": "4  Binary Data",
    "section": "",
    "text": "4.1 Logistic models for Binary Data\nUse the speed accuracy data that we use in the next chapter.\nTODO.",
    "crumbs": [
      "Scales and Choices",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Binary Data</span>"
    ]
  },
  {
    "objectID": "4a_rt_descriptive.html",
    "href": "4a_rt_descriptive.html",
    "title": "5  Descriptive Models",
    "section": "",
    "text": "5.1 The Data\nFor this chapter, we will be using the data from Wagenmakers et al. (2008) - Experiment 1 (also reanalyzed by Heathcote and Love 2012), that contains responses and response times for several participants in two conditions (where instructions emphasized either speed or accuracy). Using the same procedure as the authors, we excluded all trials with uninterpretable response time, i.e., responses that are too fast (&lt;180 ms) or too slow (&gt;2 sec instead of &gt;3 sec, see Thériault et al. 2024 for a discussion on outlier removal).\nusing Downloads, CSV, DataFrames, Random\nusing Turing, Distributions, StatsFuns, SequentialSamplingModels\nusing GLMakie\n\nRandom.seed!(123)  # For reproducibility\n\ndf = CSV.read(Downloads.download(\"https://raw.githubusercontent.com/DominiqueMakowski/CognitiveModels/main/data/wagenmakers2008.csv\"), DataFrame)\n\n# Show 10 first rows\nfirst(df, 10)\n\n10×5 DataFrame\n\n\n\nRow\nParticipant\nCondition\nRT\nError\nFrequency\n\n\n\nInt64\nString15\nFloat64\nBool\nString15\n\n\n\n\n1\n1\nSpeed\n0.7\nfalse\nLow\n\n\n2\n1\nSpeed\n0.392\ntrue\nVery Low\n\n\n3\n1\nSpeed\n0.46\nfalse\nVery Low\n\n\n4\n1\nSpeed\n0.455\nfalse\nVery Low\n\n\n5\n1\nSpeed\n0.505\ntrue\nLow\n\n\n6\n1\nSpeed\n0.773\nfalse\nHigh\n\n\n7\n1\nSpeed\n0.39\nfalse\nHigh\n\n\n8\n1\nSpeed\n0.587\ntrue\nLow\n\n\n9\n1\nSpeed\n0.603\nfalse\nLow\n\n\n10\n1\nSpeed\n0.435\nfalse\nHigh\nIn the previous chapter, we modelled the error rate (the probability of making an error) using a logistic model, and observed that it was higher in the \"Speed\" condition. But how about speed? We are going to first take interest in the RT of Correct answers only (as we can assume that errors are underpinned by a different generative process).\nAfter filtering out the errors, we create a new column, Accuracy, which is the “binarization” of the Condition column, and is equal to 1 when the condition is \"Accuracy\" and 0 when it is \"Speed\".\nCode\ndf = df[df.Error .== 0, :]\ndf.Accuracy = df.Condition .== \"Accuracy\"\nCode\nfunction plot_distribution(df, title=\"Empirical Distribution of Data from Wagenmakers et al. (2018)\")\n    fig = Figure()\n    ax = Axis(fig[1, 1], title=title,\n        xlabel=\"RT (s)\",\n        ylabel=\"Distribution\",\n        yticksvisible=false,\n        xticksvisible=false,\n        yticklabelsvisible=false)\n    Makie.density!(df[df.Condition .== \"Speed\", :RT], color=(\"#EF5350\", 0.7), label = \"Speed\")\n    Makie.density!(df[df.Condition .== \"Accuracy\", :RT], color=(\"#66BB6A\", 0.7), label = \"Accuracy\")\n    Makie.axislegend(\"Condition\"; position=:rt)\n    Makie.ylims!(ax, (0, nothing))\n    return fig\nend\n\nplot_distribution(df, \"Empirical Distribution of Data from Wagenmakers et al. (2018)\")\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie C:\\Users\\domma\\.julia\\packages\\Makie\\VRavR\\src\\scenes.jl:220",
    "crumbs": [
      "Reaction Times",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Models</span>"
    ]
  },
  {
    "objectID": "4a_rt_descriptive.html#the-data",
    "href": "4a_rt_descriptive.html#the-data",
    "title": "5  Descriptive Models",
    "section": "",
    "text": "Code Tip\n\n\n\nNote the usage of vectorization .== as we want to compare each element of the Condition vector to the target \"Accuracy\".",
    "crumbs": [
      "Reaction Times",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Models</span>"
    ]
  },
  {
    "objectID": "4a_rt_descriptive.html#gaussian-aka-linear-model",
    "href": "4a_rt_descriptive.html#gaussian-aka-linear-model",
    "title": "5  Descriptive Models",
    "section": "5.2 Gaussian (aka Linear) Model",
    "text": "5.2 Gaussian (aka Linear) Model\n\n\n\n\n\n\nNote\n\n\n\nNote that until the last section of this chapter, we will disregard the existence of multiple participants (which require the inclusion of random effects in the model). We will treat the data as if it was a single participant at first to better understand the parameters, but will show how to add random effects at the end.\n\n\nA linear model is the most common type of model. It aims at predicting the mean mu \\(\\mu\\) of the outcome variable using a Normal (aka Gaussian) distribution for the residuals. In other words, it models the outcome \\(y\\) as a Normal distribution with a mean mu \\(\\mu\\) that is itself the result of a linear function of the predictors \\(X\\) and a variance sigma \\(\\sigma\\) that is constant across all values of the predictors. It can be written as \\(y = Normal(\\mu, \\sigma)\\), where \\(\\mu = intercept + slope * X\\).\nIn order to fit a Linear Model for RTs, we need to set a prior on all these parameters, namely:\n\nSigma \\(\\sigma\\) : The variance (corresponding to the “spread” of RTs)\nMu \\(\\mu\\) : The mean for the intercept (i.e., at the reference condition which is in our case \"Speed\")\nThe effect of the condition (the slope) on the mean (\\(\\mu\\)) RT.\n\n\n5.2.1 Model Specification\n\n@model function model_Gaussian(rt; condition=nothing)\n\n    # Prior on variance \n    σ ~ truncated(Normal(0, 0.5); lower=0)  # Strictly positive half normal distribution\n\n    # Priors on intercept and effect of condition\n    μ_intercept ~ truncated(Normal(0, 1); lower=0)\n    μ_condition ~ Normal(0, 0.3)\n\n    # Iterate through every observation\n    for i in 1:length(rt)\n        # Apply formula\n        μ = μ_intercept + μ_condition * condition[i]\n        # Likelihood family\n        rt[i] ~ Normal(μ, σ)\n    end\nend\n\n# Fit the model with the data\nfit_Gaussian = model_Gaussian(df.RT; condition=df.Accuracy)\n# Sample results using MCMC\nchain_Gaussian = sample(fit_Gaussian, NUTS(), 400)\n\n\n# Summary (95% CI)\nhpd(chain_Gaussian; alpha=0.05)\n\n\nHPD\n   parameters     lower     upper \n       Symbol   Float64   Float64 \n            σ    0.1652    0.1701\n  μ_intercept    0.5071    0.5168\n  μ_condition    0.1319    0.1457\n\n\n\n\nThe effect of Condition is significant, people are on average slower (higher RT) when condition is \"Accuracy\". But is our model good?\n\n\n5.2.2 Posterior Predictive Check\n\n\nCode\npred = predict(model_Gaussian([(missing) for i in 1:length(df.RT)], condition=df.Accuracy), chain_Gaussian)\npred = Array(pred)\n\n\n\n\nCode\nfig = plot_distribution(df, \"Predictions made by Gaussian (aka Linear) Model\")\nfor i in 1:length(chain_Gaussian)\n    lines!(Makie.KernelDensity.kde(pred[:, i]), color=ifelse(df.Accuracy[i] == 1, \"#388E3C\", \"#D32F2F\"), alpha=0.1)\nend\nfig\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie C:\\Users\\domma\\.julia\\packages\\Makie\\VRavR\\src\\scenes.jl:220\n\n\n\n\n\nAs you can see, the linear models are good at predicting the mean RT (the center of the distribution), but they are not good at capturing the spread and the shape of the data.",
    "crumbs": [
      "Reaction Times",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Models</span>"
    ]
  },
  {
    "objectID": "4a_rt_descriptive.html#scaled-gaussian-model",
    "href": "4a_rt_descriptive.html#scaled-gaussian-model",
    "title": "5  Descriptive Models",
    "section": "5.3 Scaled Gaussian Model",
    "text": "5.3 Scaled Gaussian Model\nThe previous model, despite its poor fit to the data, suggests that the mean RT is higher for the Accuracy condition. But it seems like the green distribution is also wider (i.e., the response time is more variable), which is not captured by the our model (the predicted distributions have the same widths). This is expected, as typical linear models estimate only one value for sigma \\(\\sigma\\) for the whole model, hence the requirement for homoscedasticity.\n\n\n\n\n\n\nNote\n\n\n\nHomoscedasticity, or homogeneity of variances, is the assumption of similar variances accross different values of predictors. It is important in linear models as only one value for sigma \\(\\sigma\\) is estimated.\n\n\nIs it possible to set sigma \\(\\sigma\\) as a parameter that would depend on the condition, in the same way as mu \\(\\mu\\)? In Julia, this is very simple.\nAll we need is to set sigma \\(\\sigma\\) as the result of a linear function, such as \\(\\sigma = intercept + slope * condition\\). This means setting a prior on the intercept of sigma \\(\\sigma\\) (in our case, the variance in the reference condition) and a prior on how much this variance changes for the other condition. This change can, by definition, be positive or negative (i.e., the other condition can have either a biggger or a smaller variance), so the prior over the effect of condition should ideally allow for positive and negative values (e.g., σ_condition ~ Normal(0, 0.1)).\nBut this leads to an important problem.\n\n\n\n\n\n\nImportant\n\n\n\nThe combination of an intercept and a (possible negative) slope for sigma \\(\\sigma\\) technically allows for negative variance values, which is impossible (distributions cannot have a negative variance). This issue is one of the most important to address when setting up complex models for RTs.\n\n\nIndeed, even if we set a very narrow prior on the intercept of sigma \\(\\sigma\\) to fix it at for instance 0.14, and a narrow prior on the effect of condition, say \\(Normal(0, 0.001)\\), an effect of condition of -0.15 is still possible (albeit with very low probability). And such effect would lead to a sigma \\(\\sigma\\) of 0.14 - 0.15 = -0.01, which would lead to an error (and this will often happen as the sampling process does explore unlikely regions of the parameter space).\n\n5.3.1 Solution 1: Directional Effect of Condition\nOne possible (but not recommended) solution is to simply make it impossible for the effect of condition to be negative by Truncating the prior to a lower bound of 0. This can work in our case, because we know that the comparison condition is likely to have a higher variance than the reference condition (the intercept) - and if it wasn’t the case, we could have changed the reference factor. However, this is not a good practice as we are enforcing a very strong a priori specific direction of the effect, which is not always justified.\n\n@model function model_ScaledlGaussian(rt; condition=nothing)\n\n    # Priors\n    μ_intercept ~ truncated(Normal(0, 1); lower=0)\n    μ_condition ~ Normal(0, 0.3)\n\n    σ_intercept ~ truncated(Normal(0, 0.5); lower=0)  # Same prior as previously\n    σ_condition ~ truncated(Normal(0, 0.1); lower=0)  # Enforce positivity\n\n    for i in 1:length(rt)\n        μ = μ_intercept + μ_condition * condition[i]\n        σ = σ_intercept + σ_condition * condition[i]\n        rt[i] ~ Normal(μ, σ)\n    end\nend\n\nfit_ScaledlGaussian = model_ScaledlGaussian(df.RT; condition=df.Accuracy)\nchain_ScaledGaussian = sample(fit_ScaledlGaussian, NUTS(), 400)\n\n\n# Summary (95% CI)\nhpd(chain_ScaledGaussian; alpha=0.05)\n\n\nHPD\n   parameters     lower     upper \n       Symbol   Float64   Float64 \n  μ_intercept    0.5081    0.5148\n  μ_condition    0.1330    0.1446\n  σ_intercept    0.1219    0.1271\n  σ_condition    0.0714    0.0810\n\n\n\n\nWe can see that the effect of condition on sigma \\(\\sigma\\) is significantly positive: the variance is higher in the Accuracy condition as compared to the Speed condition.\n\n\n5.3.2 Solution 2: Avoid Exploring Negative Variance Values\nThe other trick is to force the sampling algorithm to avoid exploring negative variance values (when sigma \\(\\sigma\\) &lt; 0). This can be done by adding a conditional statement when sigma \\(\\sigma\\) is negative to avoid trying this value and erroring, and instead returning an infinitely low model probability (-Inf) to push away the exploration of this impossible region.\n\n@model function model_ScaledlGaussian(rt; condition=nothing)\n\n    # Priors\n    μ_intercept ~ truncated(Normal(0, 1); lower=0)\n    μ_condition ~ Normal(0, 0.3)\n\n    σ_intercept ~ truncated(Normal(0, 0.5); lower=0)\n    σ_condition ~ Normal(0, 0.1)\n\n    for i in 1:length(rt)\n        μ = μ_intercept + μ_condition * condition[i]\n        σ = σ_intercept + σ_condition * condition[i]\n        if σ &lt; 0  # Avoid negative variance values\n            Turing.@addlogprob! -Inf\n            return nothing\n        end\n        rt[i] ~ Normal(μ, σ)\n    end\nend\n\nfit_ScaledlGaussian = model_ScaledlGaussian(df.RT; condition=df.Accuracy)\nchain_ScaledGaussian = sample(fit_ScaledlGaussian, NUTS(), 400)\n\n\nhpd(chain_ScaledGaussian; alpha=0.05)\n\n\nHPD\n   parameters     lower     upper \n       Symbol   Float64   Float64 \n  μ_intercept    0.5076    0.5148\n  μ_condition    0.1316    0.1444\n  σ_intercept    0.1223    0.1273\n  σ_condition    0.0709    0.0803\n\n\n\n\n\n\n5.3.3 Solution 3: Use a “Softplus” Function\n\nExponential Transformation\nUsing the previous solution feels like a “hack” and a workaround a misspecified model. One alternative approach is to apply a function to sigma \\(\\sigma\\) to “transform” it into a positive value. We have seen example of applying “non-identity” link functions in the previous chapters with the logistic function that transforms any value between \\(-\\infty\\) and \\(+\\infty\\) into a value between 0 and 1.\nWhat function could we use to transform any values of sigma \\(\\sigma\\) into stricly positive values? One option that has been used is to express the parameter on the log-scale (which can include negative values) for priors and effects, and apply an “exponential” transformation to the parameter at the end.\nThe issue with the log link (i.e., expressing parameters on the log-scale and then transforming them using the exponential function) is that 1) it quickly generates very big numbers (which can slow down sampling efficiency), 2) The interpretation of the parameters and effects are not linear, which can add up to the complexity, and 3) normal priors on the log scale lead to a sharp peak in “real” values that can be problematic.\n\n\nCode\nxaxis = range(-6, 6, length=1000)\n\nfig = Figure()\nax1 = Axis(fig[1:2, 1], xlabel=\"Value of σ on the log scale\", ylabel=\"Actual value of σ\", title=\"Exponential function\")\nlines!(ax1, xaxis, exp.(xaxis), color=:red, linewidth=2)\nax2 = Axis(fig[1, 2], xlabel=\"Value of σ on the log scale\", ylabel=\"Plausibility\", title=\"Prior for σ ~ Normal(0, 1)\", yticksvisible=false, yticklabelsvisible=false,)\nlines!(ax2, xaxis, pdf.(Normal(0, 1), xaxis), color=:blue, linewidth=2)\nax3 = Axis(fig[2, 2], xlabel=\"Value of σ after exponential transformation\", ylabel=\"Plausibility\", yticksvisible=false, yticklabelsvisible=false,)\nlines!(ax3, exp.(xaxis), pdf.(Normal(0, 1), xaxis), color=:green, linewidth=2)\nxlims!(ax3, (-1, 40))\nfig\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie C:\\Users\\domma\\.julia\\packages\\Makie\\VRavR\\src\\scenes.jl:220\n\n\n\n\n\n\n\nSoftplus Function\nPopularized by the machine learning field, the Softplus function is an interesting alternative (see Wiemann, Kneib, and Hambuckers 2023). It is defined as \\(softplus(x) = \\log(1 + \\exp(x))\\) and its main benefit is to approximate an “identity” link for larger values (i.e., a linear relationship), only impacting negative values and values close to 0 (where the link is not linear).\n\n\nCode\nxaxis = range(-6, 6, length=1000)\n\nfig = Figure()\nax1 = Axis(fig[1:2, 1], xlabel=\"Value of σ before transformation\", ylabel=\"Actual value of σ\", title=\"Softplus function\")\nablines!(ax1, [0], [1], color=:black, linestyle=:dash)\nlines!(ax1, xaxis, softplus.(xaxis), color=:red, linewidth=2)\nax2 = Axis(fig[1, 2], xlabel=\"Value of σ before transformation\", ylabel=\"Plausibility\", title=\"Prior for σ ~ Normal(0, 1)\", yticksvisible=false, yticklabelsvisible=false,)\nlines!(ax2, xaxis, pdf.(Normal(0, 1), xaxis), color=:blue, linewidth=2)\nax3 = Axis(fig[2, 2], xlabel=\"Value of σ after softplus transformation\", ylabel=\"Plausibility\", yticksvisible=false, yticklabelsvisible=false,)\nlines!(ax3, softplus.(xaxis), pdf.(Normal(0, 1), xaxis), color=:green, linewidth=2)\nfig\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie C:\\Users\\domma\\.julia\\packages\\Makie\\VRavR\\src\\scenes.jl:220\n\n\n\n\n\n\n\nThe Model\nLet us apply the Softplus transformation (available from the StatsFuns package) to the sigma \\(\\sigma\\) parameter.\n\n@model function model_ScaledlGaussian(rt; condition=nothing)\n\n    # Priors\n    μ_intercept ~ truncated(Normal(0, 1); lower=0)\n    μ_condition ~ Normal(0, 0.3)\n\n    σ_intercept ~ Normal(0, 1)\n    σ_condition ~ Normal(0, 0.3)\n\n    for i in 1:length(rt)\n        μ = μ_intercept + μ_condition * condition[i]\n        σ = σ_intercept + σ_condition * condition[i]\n        rt[i] ~ Normal(μ, softplus(σ))\n    end\nend\n\nfit_ScaledlGaussian = model_ScaledlGaussian(df.RT; condition=df.Accuracy)\nchain_ScaledGaussian = sample(fit_ScaledlGaussian, NUTS(), 400)\n\n\nhpd(chain_ScaledGaussian; alpha=0.05)\n\n\nHPD\n   parameters     lower     upper \n       Symbol   Float64   Float64 \n  μ_intercept    0.5082    0.5144\n  μ_condition    0.1317    0.1447\n  σ_intercept   -2.0420   -1.9979\n  σ_condition    0.4804    0.5405\n\n\n\n\n\n\n\n\n\n\nCode Tip\n\n\n\nNote that one can use call the softplus() to transform the parameter back to the original scale, which can be useful for negative or small values (as for larger values, it becomes a 1-to-1 relationship).\n\nσ_condition0 = mean(chain_ScaledGaussian[:σ_intercept])\nσ_condition1 = σ_condition0 +  mean(chain_ScaledGaussian[:σ_condition])\n\nprintln(\n    \"σ for 'Speed' condition: \", round(softplus(σ_condition0); digits=4),\n    \"; σ for 'Accuracy' condition: \", round(softplus(σ_condition1); digits=4)\n)\n\nσ for 'Speed' condition: 0.1245; σ for 'Accuracy' condition: 0.1999\n\n\n\n\n\n\nConclusion\n\n\nCode\npred = predict(model_ScaledlGaussian([(missing) for i in 1:length(df.RT)], condition=df.Accuracy), chain_ScaledGaussian)\npred = Array(pred)\n\nfig = plot_distribution(df, \"Predictions made by Scaled Gaussian Model\")\nfor i in 1:length(chain_ScaledGaussian)\n    lines!(Makie.KernelDensity.kde(pred[:, i]), color=ifelse(df.Accuracy[i] == 1, \"#388E3C\", \"#D32F2F\"), alpha=0.1)\nend\nfig\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie C:\\Users\\domma\\.julia\\packages\\Makie\\VRavR\\src\\scenes.jl:220\n\n\n\n\n\nAlthough relaxing the homoscedasticity assumption is a good step forward, allowing us to make richer conclusions (e.g., the Accuracy condition leads to slower and more variable reaction times) and better capturing the data. Despite that, the Gaussian model still seem to be a poor fit to the data.",
    "crumbs": [
      "Reaction Times",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Models</span>"
    ]
  },
  {
    "objectID": "4a_rt_descriptive.html#the-problem-with-linear-models",
    "href": "4a_rt_descriptive.html#the-problem-with-linear-models",
    "title": "5  Descriptive Models",
    "section": "5.4 The Problem with Linear Models",
    "text": "5.4 The Problem with Linear Models\nReaction time (RTs) have been traditionally modeled using traditional linear models and their derived statistical tests such as t-test and ANOVAs. Importantly, linear models - by definition - will try to predict the mean of the outcome variable by estimating the “best fitting” Normal distribution. In the context of reaction times (RTs), this is not ideal, as RTs typically exhibit a non-normal distribution, skewed towards the left with a long tail towards the right. This means that the parameters of a Normal distribution (mean \\(\\mu\\) and standard deviation \\(\\sigma\\)) are not good descriptors of the data.\n\n\nLinear models try to find the best fitting Normal distribution for the data. However, for reaction times, even the best fitting Normal distribution (in red) does not capture well the actual data (in grey).\n\nA popular mitigation method to account for the non-normality of RTs is to transform the data, using for instance the popular log-transform. However, this practice should be avoided as it leads to various issues, including loss of power and distorted results interpretation (Lo and Andrews 2015; Schramm and Rouder 2019). Instead, rather than applying arbitrary data transformation, it would be better to swap the Normal distribution used by the model for a more appropriate one that can better capture the characteristics of a RT distribution.",
    "crumbs": [
      "Reaction Times",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Models</span>"
    ]
  },
  {
    "objectID": "4a_rt_descriptive.html#shifted-lognormal-model",
    "href": "4a_rt_descriptive.html#shifted-lognormal-model",
    "title": "5  Descriptive Models",
    "section": "5.5 Shifted LogNormal Model",
    "text": "5.5 Shifted LogNormal Model\nOne of the obvious candidate alternative to the log-transformation would be to use a model with a Log-transformed Normal distribution. A LogNormal distribution is a distribution of a random variable whose logarithm is normally distributed. In this model, the mean \\(\\mu\\) and is defined on the log-scale, and effects must be interpreted as multiplicative rather than additive (the condition increases the mean RT by a factor of \\(\\exp(\\mu_{condition})\\)).\nNote that for LogNormal distributions (as it is the case for many of the models introduced in the rest of the capter), the distribution parameters (\\(\\mu\\) and \\(\\sigma\\)) are not independent with respect to the mean and the standard deviation (SD). The empirical SD increases when the mean \\(\\mu\\) increases (which is seen as a feature rather than a bug, as it is consistent with typical reaction time data, Wagenmakers, Grasman, and Molenaar 2005).\nA Shifted LogNormal model introduces a shift (a delay) parameter tau \\(\\tau\\) that corresponds to the minimum “starting time” of the response process.\nWe need to set a prior for this parameter, which is usually truncated between 0 (to exclude negative minimum times) and the minimum RT of the data (the logic being that the minimum delay for response must be lower than the faster response actually observed).\nWhile \\(Uniform(0, min(RT))\\) is a common choice of prior, it is not ideal as it implies that all values between 0 and the minimum RT are equally likely, which is not the case. Indeed, psychology research has shown that such minimum response time for Humans is often betwen 100 and 250 ms. Moreover, in our case, we explicitly removed all RTs below 180 ms, suggesting that the minimum response time is more likely to approach 180 ms than 0 ms.\n\n5.5.1 Prior on Minimum RT\nInstead of a \\(Uniform\\) prior, we will use a \\(Gamma(1.1, 11)\\) distribution (truncated at min. RT), as this particular parameterization reflects the low probability of very low minimum RTs (near 0) and a steadily increasing probability for increasing times.\n\n\nCode\nxaxis = range(0, 0.3, 1000)\nfig = lines(xaxis, pdf.(Gamma(1.1, 11), xaxis); color=:blue, label=\"Gamma(1.1, 11)\")\nvlines!([minimum(df.RT)]; color=\"red\", linestyle=:dash, label=\"Min. RT = 0.18 s\")\naxislegend()\nfig\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie C:\\Users\\domma\\.julia\\packages\\Makie\\VRavR\\src\\scenes.jl:220\n\n\n\n\n\n\n\n5.5.2 Model Specification\n\n@model function model_LogNormal(rt; min_rt=minimum(df.RT), condition=nothing)\n\n    # Priors \n    τ ~ truncated(Gamma(1.1, 11); upper=min_rt)\n\n    μ_intercept ~ Normal(0, exp(1))  # On the log-scale: exp(μ) to get value in seconds\n    μ_condition ~ Normal(0, exp(0.3))\n\n    σ_intercept ~ truncated(Normal(0, 0.5); lower=0)\n    σ_condition ~ Normal(0, 0.1)\n\n    for i in 1:length(rt)\n        μ = μ_intercept + μ_condition * condition[i]\n        σ = σ_intercept + σ_condition * condition[i]\n        if σ &lt; 0  # Avoid negative variance values\n            Turing.@addlogprob! -Inf\n            return nothing\n        end\n        rt[i] ~ ShiftedLogNormal(μ, σ, τ)\n    end\nend\n\nfit_LogNormal = model_LogNormal(df.RT; condition=df.Accuracy)\nchain_LogNormal = sample(fit_LogNormal, NUTS(), 400)\n\n\n\n5.5.3 Interpretation\n\nhpd(chain_LogNormal; alpha=0.05)\n\n\nHPD\n   parameters     lower     upper \n       Symbol   Float64   Float64 \n            τ    0.1721    0.1794\n  μ_intercept   -1.1584   -1.1274\n  μ_condition    0.3123    0.3407\n  σ_intercept    0.3073    0.3232\n  σ_condition    0.0326    0.0523\n\n\n\n\n\n\nCode\npred = predict(model_LogNormal([(missing) for i in 1:length(df.RT)]; condition=df.Accuracy), chain_LogNormal)\npred = Array(pred)\n\nfig = plot_distribution(df, \"Predictions made by Shifted LogNormal Model\")\nfor i in 1:length(chain_LogNormal)\n    lines!(Makie.KernelDensity.kde(pred[:, i]), color=ifelse(df.Accuracy[i] == 1, \"#388E3C\", \"#D32F2F\"), alpha=0.1)\nend\nfig\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie C:\\Users\\domma\\.julia\\packages\\Makie\\VRavR\\src\\scenes.jl:220\n\n\n\n\n\nThis model provides a much better fit to the data, and confirms that the Accuracy condition is associated with higher RTs and higher variability (i.e., a larger distribution width).\n\n\n\n\n\n\nLogNormal distributions in nature\n\n\n\nThe reason why the Normal distribution is so ubiquituous in nature (and hence used as a good default) is due to the Central Limit Theorem, which states that the sum of a large number of independent random variables will be approximately normally distributed. Because many things in nature are the result of the addition of many random processes, the Normal distribution is very common in real life.\nHowever, it turns out that the multiplication of random variables result in a LogNormal distribution, and multiplicating (rather than additive) cascades of processes are also very common in nature, from lengths of latent periods of infectious diseases to distribution of mineral resources in the Earth’s crust, and the elemental mechanisms at stakes in physics and cell biolody (Limpert, Stahel, and Abbt 2001).\nThus, using LogNormal distributions for RTs can be justified with the assumption that response times are the result of multiplicative stochastic processes happening in the brain.",
    "crumbs": [
      "Reaction Times",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Models</span>"
    ]
  },
  {
    "objectID": "4a_rt_descriptive.html#exgaussian-model",
    "href": "4a_rt_descriptive.html#exgaussian-model",
    "title": "5  Descriptive Models",
    "section": "5.6 ExGaussian Model",
    "text": "5.6 ExGaussian Model\nAnother popular model to describe RTs uses the ExGaussian distribution, i.e., the Exponentially-modified Gaussian distribution (Balota and Yap 2011; Matzke and Wagenmakers 2009).\nThis distribution is a convolution of normal and exponential distributions and has three parameters, namely mu \\(\\mu\\) and sigma \\(\\sigma\\) - the mean and standard deviation of the Gaussian distribution - and tau \\(\\tau\\) - the exponential component of the distribution (note that although denoted by the same letter, it does not correspond directly to a shift of the distribution). Intuitively, these parameters reflect the centrality, the width and the tail dominance, respectively.\n\nBeyond the descriptive value of these types of models, some have tried to interpret their parameters in terms of cognitive mechanisms, arguing for instance that changes in the Gaussian components (\\(\\mu\\) and \\(\\sigma\\)) reflect changes in attentional processes [e.g., “the time required for organization and execution of the motor response”; Hohle (1965)], whereas changes in the exponential component (\\(\\tau\\)) reflect changes in intentional (i.e., decision-related) processes (Kieffaber et al. 2006). However, Matzke and Wagenmakers (2009) demonstrate that there is likely no direct correspondence between ex-Gaussian parameters and cognitive mechanisms, and underline their value primarily as descriptive tools, rather than models of cognition per se.\nDescriptively, the three parameters can be interpreted as:\n\nMu \\(\\mu\\) : The location / centrality of the RTs. Would correspond to the mean in a symmetrical distribution.\nSigma \\(\\sigma\\) : The variability and dispersion of the RTs. Akin to the standard deviation in normal distributions.\nTau \\(\\tau\\) : Tail weight / skewness of the distribution.\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that these parameters are not independent with respect to distribution characteristics, such as the empirical mean and SD. Below is an example of different distributions with the same location (mu \\(\\mu\\)) and dispersion (sigma \\(\\sigma\\)) parameters. Although only the tail weight parameter (tau \\(\\tau\\)) is changed, the whole distribution appears to shift is centre of mass. Hence, one should be careful note to interpret the values of mu \\(\\mu\\) directly as the “mean” or the distribution peak and sigma \\(\\sigma\\) as the SD or the “width”.\n\n\n\n\n5.6.1 Conditional Tau \\(\\tau\\) Parameter\nIn the same way as we modeled the effect of the condition on the variance component sigma \\(\\sigma\\), we can do the same for any other parameters, including the exponential component tau \\(\\tau\\). All wee need is to set a prior on the intercept and the condition effect, and make sure that \\(\\tau &gt; 0\\).\n\n@model function model_ExGaussian(rt; condition=nothing)\n\n    # Priors \n    μ_intercept ~ Normal(0, 1) \n    μ_condition ~ Normal(0, 0.3)\n\n    σ_intercept ~ truncated(Normal(0, 0.5); lower=0)\n    σ_condition ~ Normal(0, 0.1)\n\n    τ_intercept ~ truncated(Normal(0, 0.5); lower=0)\n    τ_condition ~ Normal(0, 0.1)\n\n    for i in 1:length(rt)\n        μ = μ_intercept + μ_condition * condition[i]\n        σ = σ_intercept + σ_condition * condition[i]\n        if σ &lt; 0  # Avoid negative variance values\n            Turing.@addlogprob! -Inf\n            return nothing\n        end\n        τ = τ_intercept + τ_condition * condition[i]\n        if τ &lt;= 0  # Avoid negative tau values\n            Turing.@addlogprob! -Inf\n            return nothing\n        end\n        rt[i] ~ ExGaussian(μ, σ, τ)\n    end\nend\n\nfit_ExGaussian = model_ExGaussian(df.RT; condition=df.Accuracy)\nchain_ExGaussian = sample(fit_ExGaussian, NUTS(), 400)\n\n\n\n5.6.2 Interpretation\n\nhpd(chain_ExGaussian; alpha=0.05)\n\n\nHPD\n   parameters     lower     upper \n       Symbol   Float64   Float64 \n  μ_intercept    0.4002    0.4059\n  μ_condition    0.0627    0.0733\n  σ_intercept    0.0379    0.0424\n  σ_condition    0.0101    0.0186\n  τ_intercept    0.1047    0.1126\n  τ_condition    0.0631    0.0788\n\n\n\n\n\n\nCode\npred = predict(model_ExGaussian([(missing) for i in 1:length(df.RT)]; condition=df.Accuracy), chain_ExGaussian)\npred = Array(pred)\n\nfig = plot_distribution(df, \"Predictions made by Shifted LogNormal Model\")\nfor i in 1:length(chain_ExGaussian)\n    lines!(Makie.KernelDensity.kde(pred[:, i]), color=ifelse(df.Accuracy[i] == 1, \"#388E3C\", \"#D32F2F\"), alpha=0.1)\nend\nfig\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie C:\\Users\\domma\\.julia\\packages\\Makie\\VRavR\\src\\scenes.jl:220\n\n\n\n\n\nThe ExGaussian model also provides an excellent fit to the data. Moreover, by modeling more parameters (including tau \\(\\tau\\)), we can draw more nuanced conclusions. In this case, the Accuracy condition is associated with higher RTs, higher variability, and a heavier tail (i.e., more extreme values).",
    "crumbs": [
      "Reaction Times",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Models</span>"
    ]
  },
  {
    "objectID": "4a_rt_descriptive.html#shifted-wald-model",
    "href": "4a_rt_descriptive.html#shifted-wald-model",
    "title": "5  Descriptive Models",
    "section": "5.7 Shifted Wald Model",
    "text": "5.7 Shifted Wald Model\nThe Wald distribution, also known as the Inverse Gaussian distribution, corresponds to the distribution of the first passage time of a Wiener process with a drift rate \\(\\mu\\) and a diffusion rate \\(\\sigma\\). While we will unpack this definition below and emphasize its important consequences, one can first note that it has been described as a potential model for RTs when convoluted with an exponential distribution (in the same way that the ExGaussian distribution is a convolution of a Gaussian and an exponential distribution). However, this Ex-Wald model (Schwarz 2001) was shown to be less appropriate than one of its variant, the Shifted Wald distribution (Heathcote 2004; Anders et al. 2016).\nNote that the Wald distribution, similarly to the models that we will be covering next (the “generative” models), is different from the previous distributions in that it is not characterized by a “location” and “scale” parameters (mu \\(\\mu\\) and sigma \\(\\sigma\\)). Instead, the parameters of the Shifted Wald distribution are:\n\nNu \\(\\nu\\) : A drift parameter, corresponding to the strength of the evidence accumulation process.\nAlpha \\(\\alpha\\) : A threshold parameter, corresponding to the amount of evidence required to make a decision.\nTau \\(\\tau\\) : A delay parameter, corresponding to the non-response time (i.e., the minimum time required to process the stimulus and respond). A shift parameter similar to the one in the Shifted LogNormal model.\n\n\nAs we can see, these parameters do not have a direct correspondence with the mean and standard deviation of the distribution. Their interpretation is more complex but, as we will see below, offers a window to a new level of interpretation.\n\n\n\n\n\n\nNote\n\n\n\nExplanations regarding these new parameters will be provided in the next chapter.\n\n\n\n5.7.1 Model Specification\n\n@model function model_Wald(rt; min_rt=minimum(df.RT), condition=nothing)\n\n    # Priors \n    ν_intercept ~ truncated(Normal(1, 3); lower=0)\n    ν_condition ~ Normal(0, 1)\n\n    α_intercept ~ truncated(Normal(0, 1); lower=0)\n    α_condition ~ Normal(0, 0.5)\n\n    τ_intercept ~ truncated(Gamma(1.1, 11); upper=min_rt)\n    τ_condition ~ Normal(0, 0.01)\n\n    for i in 1:length(rt)\n        ν = ν_intercept + ν_condition * condition[i]\n        if ν &lt;= 0  # Avoid negative drift\n            Turing.@addlogprob! -Inf\n            return nothing\n        end\n        α = α_intercept + α_condition * condition[i]\n        if α &lt;= 0  # Avoid negative variance values\n            Turing.@addlogprob! -Inf\n            return nothing\n        end\n        τ = τ_intercept + τ_condition * condition[i]\n        if τ &lt; 0  # Avoid negative tau values\n            Turing.@addlogprob! -Inf\n            return nothing\n        end\n        rt[i] ~ Wald(ν, α, τ)\n    end\nend\n\nfit_Wald = model_Wald(df.RT; condition=df.Accuracy)\nchain_Wald = sample(fit_Wald, NUTS(), 600)\n\n\nhpd(chain_Wald; alpha=0.05)\n\n\nHPD\n   parameters     lower     upper \n       Symbol   Float64   Float64 \n  ν_intercept    5.0817    5.3248\n  ν_condition   -1.3539   -1.0550\n  α_intercept    1.6585    1.7402\n  α_condition    0.2191    0.3506\n  τ_intercept    0.1818    0.1870\n  τ_condition   -0.0363   -0.0233\n\n\n\n\n\n\nCode\npred = predict(model_Wald([(missing) for i in 1:length(df.RT)]; condition=df.Accuracy), chain_Wald)\npred = Array(pred)\n\nfig = plot_distribution(df, \"Predictions made by Shifted Wald Model\")\nfor i in 1:length(chain_Wald)\n    lines!(Makie.KernelDensity.kde(pred[:, i]), color=ifelse(df.Accuracy[i] == 1, \"#388E3C\", \"#D32F2F\"), alpha=0.1)\nend\nfig\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie C:\\Users\\domma\\.julia\\packages\\Makie\\VRavR\\src\\scenes.jl:220\n\n\n\n\n\n\n\n5.7.2 Model Comparison\nAt this stage, given the multiple options avaiable to model RTs, you might be wondering which model is the best. One can compare the models using the Leave-One-Out Cross-Validation (LOO-CV) method, which is a Bayesian method to estimate the out-of-sample predictive accuracy of a model.\n\n\nCode\nusing ParetoSmooth\n\nloo_Gaussian = psis_loo(fit_Gaussian, chain_Gaussian, source=\"mcmc\")\nloo_ScaledGaussian = psis_loo(fit_ScaledlGaussian, chain_ScaledGaussian, source=\"mcmc\")\nloo_LogNormal = psis_loo(fit_LogNormal, chain_LogNormal, source=\"mcmc\")\nloo_ExGaussian = psis_loo(fit_ExGaussian, chain_ExGaussian, source=\"mcmc\")\nloo_Wald = psis_loo(fit_Wald, chain_Wald, source=\"mcmc\")\n\nloo_compare((\n    Gaussian = loo_Gaussian, \n    ScaledGaussian = loo_ScaledGaussian, \n    LogNormal = loo_LogNormal, \n    ExGaussian = loo_ExGaussian, \n    Wald = loo_Wald))\n\n\n\n\n\n┌────────────────┬──────────┬────────┬────────┐\n│                │  cv_elpd │ cv_avg │ weight │\n├────────────────┼──────────┼────────┼────────┤\n│     ExGaussian │     0.00 │   0.00 │   1.00 │\n│      LogNormal │  -323.67 │  -0.03 │   0.00 │\n│           Wald │  -381.68 │  -0.04 │   0.00 │\n│ ScaledGaussian │ -2466.17 │  -0.26 │   0.00 │\n│       Gaussian │ -2974.91 │  -0.31 │   0.00 │\n└────────────────┴──────────┴────────┴────────┘\n\n\nThe loo_compare() function orders models from best to worse based on their ELPD (Expected Log Pointwise Predictive Density) and provides the difference in ELPD between the best model and the other models. As one can see, traditional linear models perform terribly.",
    "crumbs": [
      "Reaction Times",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Models</span>"
    ]
  },
  {
    "objectID": "4a_rt_descriptive.html#shifted-lognormal-mixed-model",
    "href": "4a_rt_descriptive.html#shifted-lognormal-mixed-model",
    "title": "5  Descriptive Models",
    "section": "5.8 Shifted LogNormal Mixed Model",
    "text": "5.8 Shifted LogNormal Mixed Model\n\nComplex example walkthrough. Add Random effects.\n\n\n\n\nAnders, Royce, F Alario, Leendert Van Maanen, et al. 2016. “The Shifted Wald Distribution for Response Time Data Analysis.” Psychological Methods 21 (3): 309.\n\n\nBalota, David A, and Melvin J Yap. 2011. “Moving Beyond the Mean in Studies of Mental Chronometry: The Power of Response Time Distributional Analyses.” Current Directions in Psychological Science 20 (3): 160–66.\n\n\nHeathcote, Andrew. 2004. “Fitting Wald and Ex-Wald Distributions to Response Time Data: An Example Using Functions for the s-PLUS Package.” Behavior Research Methods, Instruments, & Computers 36: 678–94.\n\n\nHeathcote, Andrew, and Jonathon Love. 2012. “Linear Deterministic Accumulator Models of Simple Choice.” Frontiers in Psychology 3: 292.\n\n\nHohle, Raymond H. 1965. “Inferred Components of Reaction Times as Functions of Foreperiod Duration.” Journal of Experimental Psychology 69 (4): 382.\n\n\nKieffaber, Paul D, Emily S Kappenman, Misty Bodkins, Anantha Shekhar, Brian F O’Donnell, and William P Hetrick. 2006. “Switch and Maintenance of Task Set in Schizophrenia.” Schizophrenia Research 84 (2-3): 345–58.\n\n\nLimpert, Eckhard, Werner A Stahel, and Markus Abbt. 2001. “Log-Normal Distributions Across the Sciences: Keys and Clues: On the Charms of Statistics, and How Mechanical Models Resembling Gambling Machines Offer a Link to a Handy Way to Characterize Log-Normal Distributions, Which Can Provide Deeper Insight into Variability and Probability—Normal or Log-Normal: That Is the Question.” BioScience 51 (5): 341–52.\n\n\nLo, Steson, and Sally Andrews. 2015. “To Transform or Not to Transform: Using Generalized Linear Mixed Models to Analyse Reaction Time Data.” Frontiers in Psychology 6: 1171.\n\n\nMatzke, Dora, and Eric-Jan Wagenmakers. 2009. “Psychological Interpretation of the Ex-Gaussian and Shifted Wald Parameters: A Diffusion Model Analysis.” Psychonomic Bulletin & Review 16: 798–817.\n\n\nSchramm, Pele, and Jeffrey N Rouder. 2019. “Are Reaction Time Transformations Really Beneficial?”\n\n\nSchwarz, Wolfgang. 2001. “The Ex-Wald Distribution as a Descriptive Model of Response Times.” Behavior Research Methods, Instruments, & Computers 33: 457–69.\n\n\nThériault, Rémi, Mattan S Ben-Shachar, Indrajeet Patil, Daniel Lüdecke, Brenton M Wiernik, and Dominique Makowski. 2024. “Check Your Outliers! An Introduction to Identifying Statistical Outliers in r with Easystats.” Behavior Research Methods 56 (4): 4162–72.\n\n\nWagenmakers, Eric-Jan, Raoul PPP Grasman, and Peter CM Molenaar. 2005. “On the Relation Between the Mean and the Variance of a Diffusion Model Response Time Distribution.” Journal of Mathematical Psychology 49 (3): 195–204.\n\n\nWagenmakers, Eric-Jan, Roger Ratcliff, Pablo Gomez, and Gail McKoon. 2008. “A Diffusion Model Account of Criterion Shifts in the Lexical Decision Task.” Journal of Memory and Language 58 (1): 140–59.\n\n\nWiemann, Paul FV, Thomas Kneib, and Julien Hambuckers. 2023. “Using the Softplus Function to Construct Alternative Link Functions in Generalized Linear Models and Beyond.” Statistical Papers, 1–26.",
    "crumbs": [
      "Reaction Times",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive Models</span>"
    ]
  },
  {
    "objectID": "4b_rt_generative.html",
    "href": "4b_rt_generative.html",
    "title": "6  Generative Models",
    "section": "",
    "text": "6.1 Evidence Accumulation\nIn the previous chapter, we introduced the Wald distribution and its parameters, nu \\(\\nu\\) (drift rate) and alpha \\(\\alpha\\) (threshold). This distribution appears to have been first derived in 1900 to model the time a stock reaches a certain price (a threshold price) for the first time, and used in 1915 by Schrödinger as the time to first passage of a threshold of a Brownian motion (i.e., a random walk).\nA random walk describes a path consisting of a succession of random steps. It has been used by Francis Galton in 1894 to illustrate the Central Limit Theorem, and is now known as the Galton Board. The Galton Board is a physical model of a random walk, where balls are dropped from the top and bounce left or right at each peg until they reach the bottom. The distribution of the final position of the balls is a normal distribution.\nIn the figure below, we can see a computer simulation illustrating the same concept, with “time” being displayed on the x-axis. All iterations start at y=0, and change by -1 or +1 at each time step, until it reaches a threshold of t = 0.7 seconds.\nRandom walks are used to model a wide range of phenomena, such as the movement of particules and molecules, the stock market, the behavior of animals and, crucially, cognitive processes. For instance, it can be used to approximate evidence accumulation: the idea that a decision maker (be it a Human or any other system) accumulates evidence in a “stochastic” (i.e., random) fashion over time until a certain threshold is reached, at which point a decision is made.",
    "crumbs": [
      "Reaction Times",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generative Models</span>"
    ]
  },
  {
    "objectID": "4b_rt_generative.html#evidence-accumulation",
    "href": "4b_rt_generative.html#evidence-accumulation",
    "title": "6  Generative Models",
    "section": "",
    "text": "Caution\n\n\n\nTODO: Replace with my own video.",
    "crumbs": [
      "Reaction Times",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generative Models</span>"
    ]
  },
  {
    "objectID": "4b_rt_generative.html#wald-distribution-revisited",
    "href": "4b_rt_generative.html#wald-distribution-revisited",
    "title": "6  Generative Models",
    "section": "6.2 Wald Distribution (Revisited)",
    "text": "6.2 Wald Distribution (Revisited)\nThis is how the Wald distribution is actually generated. It corresponds to the distribution of times that it takes for a stochastic process to reach a certain threshold \\(\\alpha\\) (a certain amount of “evidence”). The twist is that the process underlying this model is a random walk with a drift rate \\(\\nu\\), which corresponds to the average amount of evidence accumulated per unit of time. In other words, the drift rate \\(\\nu\\) is the “slope” of the evidence accumulation process, representing the strength of the evidence (or the speed by which the decision maker accumulates evidence). The threshold \\(\\alpha\\) is the amount of evidence required to reach a decision (“decision” typically meaning making a response).\n\n\nIn this figure, the red line at 0 represents the non-decision time tau \\(\\tau\\). The dotted red line corresponds to the threshold \\(\\alpha\\), and the drift rate \\(\\nu\\) is the slope of the black line. The time at which each individual random accumulator crosses the threshold forms a Wald distribution.\n\nAs you can see, the Wald distribution belongs to a family of models thata do not merely attempt at describing the empirical distributions by tweaking and convolving distributions (like the ExGaussian or LogNormal models). Instead, their parameters are characterizing the data generation process.\n\n\n\n\n\n\nImportant\n\n\n\nWhile such “generative” models offer potential insights into the cognitive processes underlying the data, they inherently make strong assumptions about said underlying process (for instance, that the data of a task can be approximated by a stochastic evidence accumulation process). It is thus crucial to always keep in mind the limitations and assumptions of the models we use. Don’t forget, with great power comes great responsability.",
    "crumbs": [
      "Reaction Times",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generative Models</span>"
    ]
  },
  {
    "objectID": "4b_rt_generative.html#drift-diffusion-model-ddm",
    "href": "4b_rt_generative.html#drift-diffusion-model-ddm",
    "title": "6  Generative Models",
    "section": "6.3 Drift Diffusion Model (DDM)",
    "text": "6.3 Drift Diffusion Model (DDM)\nInterestingly, the Wald model is actually a special case of a more general type called the Drift Diffusion Model (DDM) (named as such because the evidence accumulation is assumed to be a “diffusion” process, i.e., a random walk). One of the main difference is that in the Wald model, the drift rate \\(\\nu\\) must be positive, as it tracks the time taken by the diffusion processes to reach only one “positive” threshold \\(\\alpha\\).\nBut what happens if we relax this and allow the drift rate to be null or negative? Many traces might never reach the upper threshold, and might instead reach high “negative” values. The idea is then to also consider a lower threshold, which would correspond to a different response. Finally, one could also modulate the starting point of the process relative to the thresholds (i.e., how close to the upper or lower threshold the accumulation process starts). This is, in essence, the core idea behind the Drift Diffusion Model.\nDrift Diffusion Models are useful to jointly model RTs and a binary outcome, such as 2 different choices or accuracy (e.g., “correct” vs. “error”). They allow, in theory, to delineate various cognitive processes, such as the speed of evidence accumulation (drift rate), the amount of evidence required to make a decision (threshold), and the starting point of the accumulation process (bias).\n\nThe parameters are:\n\nNu \\(\\nu\\) : The drift rate (also sometimes denoted delta \\(\\delta\\)), representing the average slope of the accumulation process towards the boundaries. The larger the (absolute value of the) drift rate, the more effective the evidence accumulation for the corresponding response option. A drift rate close to 0 suggests an ambiguous stimulus. Typical range: [-5, 5].\nAlpha \\(\\alpha\\) : The boundary separation threshold is the distance between the two decision bounds (lower bound being at 0 and upper bound at alpha \\(\\alpha\\)). It has been interpreted as a measure of response caution (i.e., of speed-accuracy trade-off, with high alpha \\(\\alpha\\) being related to high accuracy). It represents the amount of evidence that is needed to make a response. Typical range: [0.5, 2].\nBeta \\(\\beta\\) : The initial bias towards any of the responses. The starting point of the accumulation process (in percentage of alpha \\(\\alpha\\): if \\(\\alpha = 2.0\\) and \\(\\beta = 0.5\\), then the actual starting point is \\(2.0*0.5=1\\)). Typical range: [0.3, 0.7].\nTau \\(\\tau\\) : The non-decision time. It represents all non-decisional process, such as stimulus encoding, motor processes, etc. Typical range (in seconds): [0.1, 0.5].\n\nThis basic model is sometimes referred to as a Wiener model, as expanded versions of the DDM exist with additional parameters (e.g., variability of the drift rate).",
    "crumbs": [
      "Reaction Times",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generative Models</span>"
    ]
  },
  {
    "objectID": "4b_rt_generative.html#linear-ballistic-accumulator-lba",
    "href": "4b_rt_generative.html#linear-ballistic-accumulator-lba",
    "title": "6  Generative Models",
    "section": "6.4 Linear Ballistic Accumulator (LBA)",
    "text": "6.4 Linear Ballistic Accumulator (LBA)\nTODO.",
    "crumbs": [
      "Reaction Times",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generative Models</span>"
    ]
  },
  {
    "objectID": "4b_rt_generative.html#other-models-lnr-rdm",
    "href": "4b_rt_generative.html#other-models-lnr-rdm",
    "title": "6  Generative Models",
    "section": "6.5 Other Models (LNR, RDM)",
    "text": "6.5 Other Models (LNR, RDM)\nTODO.",
    "crumbs": [
      "Reaction Times",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generative Models</span>"
    ]
  },
  {
    "objectID": "4b_rt_generative.html#including-random-effects",
    "href": "4b_rt_generative.html#including-random-effects",
    "title": "6  Generative Models",
    "section": "6.6 Including Random Effects",
    "text": "6.6 Including Random Effects\n\n6.6.1 Random Intercept\nTODO.\n\n\n6.6.2 Random Slopes\nTODO.\n\n\n6.6.3 Performance Tips\nTODO.",
    "crumbs": [
      "Reaction Times",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generative Models</span>"
    ]
  },
  {
    "objectID": "4b_rt_generative.html#additional-resources",
    "href": "4b_rt_generative.html#additional-resources",
    "title": "6  Generative Models",
    "section": "6.7 Additional Resources",
    "text": "6.7 Additional Resources\n\nLindelov’s overview of RT models: An absolute must-read.\nDe Boeck & Jeon (2019): A paper providing an overview of RT models.\nhttps://github.com/vasishth/bayescogsci\nAn expert guide to planning experimental tasks for evidence accumula6on modelling (Boag, 2024)",
    "crumbs": [
      "Reaction Times",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generative Models</span>"
    ]
  },
  {
    "objectID": "5_individual.html",
    "href": "5_individual.html",
    "title": "7  Individual Differences",
    "section": "",
    "text": "7.1 Extracting Individual Parameters From Mixed Models\nTODO.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Individual Differences</span>"
    ]
  },
  {
    "objectID": "5_individual.html#population-informed-individual-bayesian-models",
    "href": "5_individual.html#population-informed-individual-bayesian-models",
    "title": "7  Individual Differences",
    "section": "7.2 Population-informed Individual Bayesian Models",
    "text": "7.2 Population-informed Individual Bayesian Models\nTODO.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Individual Differences</span>"
    ]
  },
  {
    "objectID": "5_individual.html#task-reliability",
    "href": "5_individual.html#task-reliability",
    "title": "7  Individual Differences",
    "section": "7.3 Task reliability",
    "text": "7.3 Task reliability\n\nSignal to Noise Ratio (https://realitybending.github.io/post/2024-03-18-signaltonoisemixed/)\nhttps://journals.sagepub.com/doi/10.1177/09637214231220923\nCronbach’s merger\nhttps://www.nature.com/articles/s44271-024-00114-4",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Individual Differences</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Anders, Royce, F Alario, Leendert Van Maanen, et al. 2016. “The\nShifted Wald Distribution for Response Time Data Analysis.”\nPsychological Methods 21 (3): 309.\n\n\nBalota, David A, and Melvin J Yap. 2011. “Moving Beyond the Mean\nin Studies of Mental Chronometry: The Power of Response Time\nDistributional Analyses.” Current Directions in Psychological\nScience 20 (3): 160–66.\n\n\nHeathcote, Andrew. 2004. “Fitting Wald and Ex-Wald Distributions\nto Response Time Data: An Example Using Functions for the s-PLUS\nPackage.” Behavior Research Methods, Instruments, &\nComputers 36: 678–94.\n\n\nHeathcote, Andrew, and Jonathon Love. 2012. “Linear Deterministic\nAccumulator Models of Simple Choice.” Frontiers in\nPsychology 3: 292.\n\n\nHohle, Raymond H. 1965. “Inferred Components of Reaction Times as\nFunctions of Foreperiod Duration.” Journal of Experimental\nPsychology 69 (4): 382.\n\n\nKieffaber, Paul D, Emily S Kappenman, Misty Bodkins, Anantha Shekhar,\nBrian F O’Donnell, and William P Hetrick. 2006. “Switch and\nMaintenance of Task Set in Schizophrenia.” Schizophrenia\nResearch 84 (2-3): 345–58.\n\n\nLimpert, Eckhard, Werner A Stahel, and Markus Abbt. 2001.\n“Log-Normal Distributions Across the Sciences: Keys and Clues: On\nthe Charms of Statistics, and How Mechanical Models Resembling Gambling\nMachines Offer a Link to a Handy Way to Characterize Log-Normal\nDistributions, Which Can Provide Deeper Insight into Variability and\nProbability—Normal or Log-Normal: That Is the Question.”\nBioScience 51 (5): 341–52.\n\n\nLo, Steson, and Sally Andrews. 2015. “To Transform or Not to\nTransform: Using Generalized Linear Mixed Models to Analyse Reaction\nTime Data.” Frontiers in Psychology 6: 1171.\n\n\nMakowski, Dominique, An Shu Te, Stephanie Kirk, Ngoi Zi Liang, and SH\nAnnabel Chen. 2023. “A Novel Visual Illusion Paradigm Provides\nEvidence for a General Factor of Illusion Sensitivity and Personality\nCorrelates.” Scientific Reports 13 (1): 6594.\n\n\nMatzke, Dora, and Eric-Jan Wagenmakers. 2009. “Psychological\nInterpretation of the Ex-Gaussian and Shifted Wald Parameters: A\nDiffusion Model Analysis.” Psychonomic Bulletin &\nReview 16: 798–817.\n\n\nSchramm, Pele, and Jeffrey N Rouder. 2019. “Are Reaction Time\nTransformations Really Beneficial?”\n\n\nSchwarz, Wolfgang. 2001. “The Ex-Wald Distribution as a\nDescriptive Model of Response Times.” Behavior Research\nMethods, Instruments, & Computers 33: 457–69.\n\n\nThériault, Rémi, Mattan S Ben-Shachar, Indrajeet Patil, Daniel Lüdecke,\nBrenton M Wiernik, and Dominique Makowski. 2024. “Check Your\nOutliers! An Introduction to Identifying Statistical Outliers in r with\nEasystats.” Behavior Research Methods 56 (4): 4162–72.\n\n\nWagenmakers, Eric-Jan, Raoul PPP Grasman, and Peter CM Molenaar. 2005.\n“On the Relation Between the Mean and the Variance of a Diffusion\nModel Response Time Distribution.” Journal of Mathematical\nPsychology 49 (3): 195–204.\n\n\nWagenmakers, Eric-Jan, Roger Ratcliff, Pablo Gomez, and Gail McKoon.\n2008. “A Diffusion Model Account of Criterion Shifts in the\nLexical Decision Task.” Journal of Memory and Language\n58 (1): 140–59.\n\n\nWiemann, Paul FV, Thomas Kneib, and Julien Hambuckers. 2023.\n“Using the Softplus Function to Construct Alternative Link\nFunctions in Generalized Linear Models and Beyond.”\nStatistical Papers, 1–26.",
    "crumbs": [
      "References"
    ]
  }
]