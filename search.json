[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cognitive Models",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#why-julia",
    "href": "index.html#why-julia",
    "title": "Cognitive Models",
    "section": "Why Julia?",
    "text": "Why Julia?\nJulia - the new cool kid on the scientific block - is a modern programming language with many benefits when compared with R or Python. Importantly, it is currently the only language in which we can fit all the cognitive models under a Bayesian framework using a unified interface like Turing and SSM.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#why-bayesian",
    "href": "index.html#why-bayesian",
    "title": "Cognitive Models",
    "section": "Why Bayesian?",
    "text": "Why Bayesian?\nUnfortunately, cognitive models often involve distributions for which Frequentist estimations are not yet implemented, and usually contain a lot of parameters (due to the presence of random effects), which makes traditional algorithms fail to converge. Simply put, the Bayesian approach is the only one currently robust enough to fit these somewhat complex models.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#the-plan",
    "href": "index.html#the-plan",
    "title": "Cognitive Models",
    "section": "The Plan",
    "text": "The Plan\nAs this is a fast-evolving field (both from the theoretical - with new models being proposed - and the technical side - with improvements to the packages and the algorithms), the book needs to be future-resilient and updatable to keep up with the latest best practices.\n\nDecide on the framework to build the book in a reproducible and collaborative manner (Quarto?)\nSet up the infrastructure to automatically build it using GitHub actions and host it on GitHub pages\nWrite the content of the book\nReferencing\n\nAdd Zenodo DOI and reference (but how to deal with evolving author? Through versioning?)\nPublish a paper to present the book project (JOSE)?",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#looking-for-coauthors",
    "href": "index.html#looking-for-coauthors",
    "title": "Cognitive Models",
    "section": "Looking for Coauthors",
    "text": "Looking for Coauthors\nThis project can only be achieved by a team, and I suspect no single person has currently all the skills and knowledge to cover all the content. We need many people who have strengths in various aspects, such as Julia/Turing, theory, writing, making plots etc. Most importantly, this project can serve as a way for us to learn more about this approach to psychological science.\nIf you are interested in the project, you can let us know by opening an issue or getting in touch.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "1_introduction.html",
    "href": "1_introduction.html",
    "title": "1  Fundamentals of Bayesian Modeling in Julia",
    "section": "",
    "text": "1.1 Very quick intro to Julia and Turing\nGoal is to teach just enough so that the reader understands the code.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Fundamentals of Bayesian Modeling in Julia</span>"
    ]
  },
  {
    "objectID": "1_introduction.html#very-quick-intro-to-julia-and-turing",
    "href": "1_introduction.html#very-quick-intro-to-julia-and-turing",
    "title": "1  Fundamentals of Bayesian Modeling in Julia",
    "section": "",
    "text": "1.1.1 Generate Data from Normal Distribution\n\n\nCode\nusing Turing, Distributions, Random\nusing Makie\n\n# Random sample from a Normal(μ=100, σ=15)\niq = rand(Normal(100, 15), 500)\n\n\n\n\nCode\nfig = Figure()\nax = Axis(fig[1, 1], title=\"Distribution\")\ndensity!(ax, iq)\nfig\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie C:\\Users\\domma\\.julia\\packages\\Makie\\VRavR\\src\\scenes.jl:220\n\n\n\n\n\n\n\n\n\n\n\n1.1.2 Recover Distribution Parameters with Turing\n\n\nCode\n@model function model_gaussian(x)\n    # Priors\n    μ ~ Uniform(0, 200)\n    σ ~ Uniform(0, 30)\n\n    # Check against each datapoint\n    for i in 1:length(x)\n        x[i] ~ Normal(μ, σ)\n    end\nend\n\nmodel = model_gaussian(iq)\nsampling_results = sample(model, NUTS(), 400)\n\n# Summary (95% CI)\nsummarystats(sampling_results)\n\n\n┌ Info: Found initial step size\n└   ϵ = 0.05\nSampling:   0%|█                                        |  ETA: 0:00:29Sampling: 100%|█████████████████████████████████████████| Time: 0:00:00\n\n\n\nSummary Statistics\n  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   e ⋯\n      Symbol   Float64   Float64   Float64    Float64    Float64   Float64     ⋯\n           μ   99.7082    0.6935    0.0367   349.6040   232.8456    1.0045     ⋯\n           σ   15.2626    0.4713    0.0255   337.7052   265.8666    1.0002     ⋯\n                                                                1 column omitted",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Fundamentals of Bayesian Modeling in Julia</span>"
    ]
  },
  {
    "objectID": "1_introduction.html#linear-models",
    "href": "1_introduction.html#linear-models",
    "title": "1  Fundamentals of Bayesian Modeling in Julia",
    "section": "1.2 Linear Models",
    "text": "1.2 Linear Models\nUnderstand what the parameters mean (intercept, slopes, sigma).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Fundamentals of Bayesian Modeling in Julia</span>"
    ]
  },
  {
    "objectID": "1_introduction.html#boostrapping",
    "href": "1_introduction.html#boostrapping",
    "title": "1  Fundamentals of Bayesian Modeling in Julia",
    "section": "1.3 Boostrapping",
    "text": "1.3 Boostrapping\nIntroduce concepts related to pseudo-posterior distribution description",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Fundamentals of Bayesian Modeling in Julia</span>"
    ]
  },
  {
    "objectID": "1_introduction.html#hierarchical-models",
    "href": "1_introduction.html#hierarchical-models",
    "title": "1  Fundamentals of Bayesian Modeling in Julia",
    "section": "1.4 Hierarchical Models",
    "text": "1.4 Hierarchical Models\nSimpson’s paradox, random effects, how to leverage them to model interindividual differences",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Fundamentals of Bayesian Modeling in Julia</span>"
    ]
  },
  {
    "objectID": "1_introduction.html#bayesian-estimation",
    "href": "1_introduction.html#bayesian-estimation",
    "title": "1  Fundamentals of Bayesian Modeling in Julia",
    "section": "1.5 Bayesian estimation",
    "text": "1.5 Bayesian estimation\nintroduce Bayesian estimation and priors over parameters",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Fundamentals of Bayesian Modeling in Julia</span>"
    ]
  },
  {
    "objectID": "1_introduction.html#bayesian-mixed-linear-regression",
    "href": "1_introduction.html#bayesian-mixed-linear-regression",
    "title": "1  Fundamentals of Bayesian Modeling in Julia",
    "section": "1.6 Bayesian mixed linear regression",
    "text": "1.6 Bayesian mixed linear regression\nput everything together",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Fundamentals of Bayesian Modeling in Julia</span>"
    ]
  },
  {
    "objectID": "2_predictors.html",
    "href": "2_predictors.html",
    "title": "2  Predictors",
    "section": "",
    "text": "2.1 Categorical predictors (Condition, Group, …)\nNested interactions, contrasts, …",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Predictors</span>"
    ]
  },
  {
    "objectID": "2_predictors.html#ordered-predictors-likert-scales",
    "href": "2_predictors.html#ordered-predictors-likert-scales",
    "title": "2  Predictors",
    "section": "2.2 Ordered predictors (Likert Scales)",
    "text": "2.2 Ordered predictors (Likert Scales)\nLikert scales, i.e., ordered multiple discrete choices are often used in surveys and questionnaires. While such data is often treated as a continuous variable, such assumption is not necessarily valid. Indeed, distance between the choices is not necessarily equal. For example, the difference between “strongly agree” and “agree” might not be the same as between “agree” and “neutral”. Even when using integers like 1, 2, 3, 4; people might implicitly process “4” as more extreme relative to “3” as “3” to “2”.\n\n\nThe probabilities assigned to discrete probability descriptors are not necessarily equidistant (https://github.com/zonination/perceptions)\n\nWhat can we do to better reflect the cognitive process underlying a Likert scale responses? Monotonic effects.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Predictors</span>"
    ]
  },
  {
    "objectID": "2_predictors.html#interactions",
    "href": "2_predictors.html#interactions",
    "title": "2  Predictors",
    "section": "2.3 Interactions",
    "text": "2.3 Interactions\nTodo.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Predictors</span>"
    ]
  },
  {
    "objectID": "2_predictors.html#non-linear-relationships-polynomial-gams",
    "href": "2_predictors.html#non-linear-relationships-polynomial-gams",
    "title": "2  Predictors",
    "section": "2.4 Non-linear relationships (polynomial, GAMs)",
    "text": "2.4 Non-linear relationships (polynomial, GAMs)\nTodo.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Predictors</span>"
    ]
  },
  {
    "objectID": "3_scales.html",
    "href": "3_scales.html",
    "title": "3  Choice and Scales",
    "section": "",
    "text": "Beta models\nOrdBeta models for slider scales\nLogistic models for binary data\n\nUse the speed accuracy data that we use in the next chapter.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Choice and Scales</span>"
    ]
  },
  {
    "objectID": "4_rt.html",
    "href": "4_rt.html",
    "title": "4  Reaction Times",
    "section": "",
    "text": "4.1 The Data\nFor this chapter, we will be using the data from Wagenmakers et al. (2008) - Experiment 1 (also reanalyzed by Heathcote and Love 2012), that contains responses and response times for several participants in two conditions (where instructions emphasized either speed or accuracy). Using the same procedure as the authors, we excluded all trials with uninterpretable response time, i.e., responses that are too fast (&lt;180 ms) or too slow (&gt;2 sec instead of &gt;3 sec, see Thériault et al. 2024 for a discussion on outlier removal).\nusing Downloads, CSV, DataFrames\nusing Turing, Distributions, SequentialSamplingModels\nusing GLMakie\n\ndf = CSV.read(Downloads.download(\"https://raw.githubusercontent.com/DominiqueMakowski/CognitiveModels/main/data/wagenmakers2008.csv\"), DataFrame)\n\n# Show 10 first rows\nfirst(df, 10)\n\n10×5 DataFrame\n\n\n\nRow\nParticipant\nCondition\nRT\nError\nFrequency\n\n\n\nInt64\nString15\nFloat64\nBool\nString15\n\n\n\n\n1\n1\nSpeed\n0.7\nfalse\nLow\n\n\n2\n1\nSpeed\n0.392\ntrue\nVery Low\n\n\n3\n1\nSpeed\n0.46\nfalse\nVery Low\n\n\n4\n1\nSpeed\n0.455\nfalse\nVery Low\n\n\n5\n1\nSpeed\n0.505\ntrue\nLow\n\n\n6\n1\nSpeed\n0.773\nfalse\nHigh\n\n\n7\n1\nSpeed\n0.39\nfalse\nHigh\n\n\n8\n1\nSpeed\n0.587\ntrue\nLow\n\n\n9\n1\nSpeed\n0.603\nfalse\nLow\n\n\n10\n1\nSpeed\n0.435\nfalse\nHigh\nIn the previous chapter, we modelled the error rate (the probability of making an error) using a logistic model, and observed that it was higher in the \"Speed\" condition. But how about speed? We are going to first take interest in the RT of Correct answers only (as we can assume that errors are underpinned by a different generative process).\nAfter filtering out the errors, we create a new column, Accuracy, which is the “binarization” of the Condition column, and is equal to 1 when the condition is \"Accuracy\" and 0 when it is \"Speed\".\nCode\ndf = df[df.Error .== 0, :]\ndf.Accuracy = df.Condition .== \"Accuracy\"\nCode\nfunction plot_distribution(df, title=\"Empirical Distribution of Data from Wagenmakers et al. (2018)\")\n    fig = Figure()\n    ax = Axis(fig[1, 1], title=title,\n        xlabel=\"RT (s)\",\n        ylabel=\"Distribution\",\n        yticksvisible=false,\n        xticksvisible=false,\n        yticklabelsvisible=false)\n    Makie.density!(df[df.Condition .== \"Speed\", :RT], color=(\"#EF5350\", 0.7), label = \"Speed\")\n    Makie.density!(df[df.Condition .== \"Accuracy\", :RT], color=(\"#66BB6A\", 0.7), label = \"Accuracy\")\n    Makie.axislegend(\"Condition\"; position=:rt)\n    Makie.ylims!(ax, (0, nothing))\n    return fig\nend\n\nplot_distribution(df, \"Empirical Distribution of Data from Wagenmakers et al. (2018)\")\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie C:\\Users\\domma\\.julia\\packages\\Makie\\VRavR\\src\\scenes.jl:220",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reaction Times</span>"
    ]
  },
  {
    "objectID": "4_rt.html#the-data",
    "href": "4_rt.html#the-data",
    "title": "4  Reaction Times",
    "section": "",
    "text": "Code Tip\n\n\n\nNote the usage of vectorization .== as we want to compare each element of the Condition vector to the target \"Accuracy\".",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reaction Times</span>"
    ]
  },
  {
    "objectID": "4_rt.html#descriptive-models",
    "href": "4_rt.html#descriptive-models",
    "title": "4  Reaction Times",
    "section": "4.2 Descriptive Models",
    "text": "4.2 Descriptive Models\n\n\n\n\n\n\nNote\n\n\n\nNote until the last section, we will disregard the existence of multiple participants (which require the inclusion of random effects in the model). We will treat the data as if it was a single participant at first to better understand the parameters, but will show how to add random effects at the end.\n\n\n\n4.2.1 Gaussian (aka Linear) Model\nA linear model is the most common type of model. It aims at predicting the mean \\(\\mu\\) of the outcome variable using a Normal (aka Gaussian) distribution for the residuals. In other words, it models the outcome \\(y\\) as a Normal distribution with a mean \\(\\mu\\) that is itself hte result of a linear function of the predictors \\(X\\) and a variance \\(\\sigma\\) that is constant across all values of the predictors. It can be written as \\(y = Normal(\\mu, \\sigma)\\), where \\(\\mu = intercept + slope * X\\).\nIn order to fit a Linear Model for RTs, we need to set a prior on all these parameters, namely: - The variance \\(\\sigma\\) (correspondong to the “spread” of RTs) - The mean \\(\\mu\\) for the intercept (i.e., at the reference condition which is in our case \"Speed\") - The effect of the condition (the slope).\n\nModel Specification\n\n@model function model_Gaussian(rt; condition=nothing)\n\n    # Set priors on variance, intercept and effect of condition\n    σ ~ truncated(Normal(0, 1); lower=0)\n\n    μ_intercept ~ truncated(Normal(0, 1); lower=0)\n    μ_condition ~ Normal(0, 0.5)\n\n    for i in 1:length(rt)\n        μ = μ_intercept + μ_condition * condition[i]\n        rt[i] ~ Normal(μ, σ)\n    end\nend\n\n\nmodel = model_Gaussian(df.RT; condition=df.Accuracy)\nchain_Gaussian = sample(model, NUTS(), 400)\n\n\n# Summary (95% CI)\nhpd(chain_Gaussian; alpha=0.05)\n\n\nHPD\n   parameters     lower     upper \n       Symbol   Float64   Float64 \n            σ    0.1653    0.1697\n  μ_intercept    0.5068    0.5160\n  μ_condition    0.1321    0.1459\n\n\n\n\nThe effect of Condition is significant, people are on average slower (higher RT) when condition is \"Accuracy\". But is our model good?\n\n\nPosterior Predictive Check\n\n\nCode\npred = predict(model_Gaussian([(missing) for i in 1:length(df.RT)], condition=df.Accuracy), chain_Gaussian)\npred = Array(pred)\n\n\n\n\nCode\nfig = plot_distribution(df, \"Predictions made by Gaussian (aka Linear) Model\")\nfor i in 1:length(chain_Gaussian)\n    lines!(Makie.KernelDensity.kde(pred[:, i]), color=ifelse(df.Accuracy[i] == 1, \"#388E3C\", \"#D32F2F\"), alpha=0.1)\nend\nfig\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie C:\\Users\\domma\\.julia\\packages\\Makie\\VRavR\\src\\scenes.jl:220\n\n\n\n\n\n\n\n\n4.2.2 Scaled Gaussian Model\nThe previous model, despite its poor fit to the data, suggests that the mean RT is higher for the Accuracy condition. But it seems like the distribution is also wider (response time is more variable). Typical linear model estimate only one value for sigma \\(\\sigma\\) for the whole model, hence the requirement for homoscedasticity.\n\n\n\n\n\n\nNote\n\n\n\nHomoscedasticity, or homogeneity of variances, is the assumption of similar variances accross different values of predictors. It is important in linear models as only one value for sigma \\(\\sigma\\) is estimated.\n\n\nIs it possible to set sigma \\(\\sigma\\) as a parameter that would depend on the condition, in the same way as mu \\(\\mu\\)? In Julia, this is very simple.\nAll we need is to set sigma \\(\\sigma\\) as the result of a linear function, such as \\(\\sigma = intercept + slope * condition\\). This means setting a prior on the intercept of sigma \\(\\sigma\\) (in our case, the variance in the reference condition) and a prior on how much this variance changes for the other condition. This change can, by definition, be positive or negative (i.e., the other condition can have either a biggger or a smaller variance), so the prior over the effect of condition should ideally allow for positive and negative values (e.g., σ_condition ~ Normal(0, 0.1)).\nBut this leads to an important problem.\n\n\n\n\n\n\nImportant\n\n\n\nThe combination of an intercept and a (possible negative) slope for sigma \\(\\sigma\\) technically allows for negative variance values, which is impossible (distributions cannot have a negative variance). This issue is one of the most important to address when setting up complex models for RTs.\n\n\nIndeed, even if we set a very narrow prior on the intercept of sigma \\(\\sigma\\) to fix it at for instance 0.14, and a narrow prior on the effect of condition, say \\(Normal(0, 0.001)\\), an effect of condition of -0.15 is still possible (albeit with very low probability). And such effect would lead to a sigma \\(\\sigma\\) of 0.14 - 0.15 = -0.01, which would lead to an error (and this will often happen as the sampling process does explore unlikely regions of the parameter space).\n\nSolution 1: Directional Effect of Condition\nOne possible (but not recommended) solution is to simply make it impossible for the effect of condition to be negative. This can work in our case, because we know that the comparison condition is likely to have a higher variance than the reference condition (the intercept) - and if it wasn’t the case, we could have changed the reference factor.\n\n@model function model_ScaledlGaussian1(rt; condition=nothing)\n\n    # Priors\n    μ_intercept ~ truncated(Normal(0, 1); lower=0)\n    μ_condition ~ Normal(0, 0.5)\n\n    σ_intercept ~ truncated(Normal(0, 1); lower=0)  # Same prior as previously\n    σ_condition ~ truncated(Normal(0, 0.1); lower=0)  # Enforce positivity\n\n    for i in 1:length(rt)\n        μ = μ_intercept + μ_condition * condition[i]\n        σ = σ_intercept + σ_condition * condition[i]\n        rt[i] ~ Normal(μ, σ)\n    end\nend\n\nmodel = model_ScaledlGaussian1(df.RT; condition=df.Accuracy)\nchain_ScaledGaussian = sample(model, NUTS(), 400)\n\n\n# Summary (95% CI)\nhpd(chain_ScaledGaussian; alpha=0.05)\n\n\nHPD\n   parameters     lower     upper \n       Symbol   Float64   Float64 \n  μ_intercept    0.5081    0.5150\n  μ_condition    0.1324    0.1446\n  σ_intercept    0.1220    0.1265\n  σ_condition    0.0716    0.0800\n\n\n\n\nWe can see that the effect of condition on sigma \\(\\sigma\\) is significantly positive: the variance is higher in the Accuracy condition as compared to the Speed condition.\n\n\nSolution 2: Avoid Exploring Negative Variance Values\nThe other trick is to force the sampling algorithm to avoid exploring negative variance values (when sigma \\(\\sigma\\) &lt; 0). This can be done by adding a conditional statement when sigma \\(\\sigma\\) is negative to avoid trying this value and erroring, and instead returning an infinitely low model probability (-Inf) to push away the exploration of this impossible region.\n\n@model function model_ScaledlGaussian(rt; condition=nothing)\n\n    # Priors\n    μ_intercept ~ truncated(Normal(0, 1); lower=0)\n    μ_condition ~ Normal(0, 0.5)\n\n    σ_intercept ~ truncated(Normal(0, 1); lower=0)\n    σ_condition ~ Normal(0, 0.1)\n\n    for i in 1:length(rt)\n        μ = μ_intercept + μ_condition * condition[i]\n        σ = σ_intercept + σ_condition * condition[i]\n        if σ &lt; 0  # Avoid negative variance values\n            Turing.@addlogprob! -Inf\n            return nothing\n        end\n        rt[i] ~ Normal(μ, σ)\n    end\nend\n\nmodel = model_ScaledlGaussian(df.RT; condition=df.Accuracy)\nchain_ScaledGaussian = sample(model, NUTS(), 400)\n\n\nhpd(chain_ScaledGaussian; alpha=0.05)\n\n\nHPD\n   parameters     lower     upper \n       Symbol   Float64   Float64 \n  μ_intercept    0.5083    0.5144\n  μ_condition    0.1308    0.1435\n  σ_intercept    0.1222    0.1268\n  σ_condition    0.0702    0.0795\n\n\n\n\n\n\nCode\npred = predict(model_ScaledlGaussian([(missing) for i in 1:length(df.RT)], condition=df.Accuracy), chain_ScaledGaussian)\npred = Array(pred)\n\nfig = plot_distribution(df, \"Predictions made by Scaled Gaussian Model\")\nfor i in 1:length(chain_ScaledGaussian)\n    lines!(Makie.KernelDensity.kde(pred[:, i]), color=ifelse(df.Accuracy[i] == 1, \"#388E3C\", \"#D32F2F\"), alpha=0.1)\nend\nfig\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie C:\\Users\\domma\\.julia\\packages\\Makie\\VRavR\\src\\scenes.jl:220\n\n\n\n\n\n\nAlthough relaxing the homoscedasticity assumption is a good step forward, allowing us to make richer conclusions and better capturing the data. Despite that, the Gaussian model stil seem to be a poor fit to the data.\n\n\n\n4.2.3 The Problem with Linear Models\nReaction time (RTs) have been traditionally modeled using traditional linear models and their derived statistical tests such as t-test and ANOVAs. Importantly, linear models - by definition - will try to predict the mean of the outcome variable by estimating the “best fitting” Normal distribution. In the context of reaction times (RTs), this is not ideal, as RTs typically exhibit a non-normal distribution, skewed towards the left with a long tail towards the right. This means that the parameters of a Normal distribution (mean \\(\\mu\\) and standard deviation \\(\\sigma\\)) are not good descriptors of the data.\n\n\nLinear models try to find the best fitting Normal distribution for the data. However, for reaction times, even the best fitting Normal distribution (in red) does not capture well the actual data (in grey).\n\nA popular mitigation method to account for the non-normality of RTs is to transform the data, using for instance the popular log-transform. However, this practice should be avoided as it leads to various issues, including loss of power and distorted results interpretation (Lo and Andrews 2015; Schramm and Rouder 2019). Instead, rather than applying arbitrary data transformation, it would be better to swap the Normal distribution used by the model for a more appropriate one that can better capture the characteristics of a RT distribution.\n\n\n4.2.4 Shifted LogNormal Model\nOne of the obvious candidate alternative to the log-transformation would be to use a model with a Log-transformed Normal distribution.\nNew parameter, \\(\\tau\\) (Tau for delay), which corresponds to the “starting time”. We need to set a prior for this parameter, which is usually truncated between 0 and the minimum RT of the data (the logic being that the minimum delay for response must be lower than the faster response actually observed).\n\n\nCode\nxaxis = range(0, 1, 1000)\nlines(xaxis, pdf.(Gamma(1.1, 11), xaxis))\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie C:\\Users\\domma\\.julia\\packages\\Makie\\VRavR\\src\\scenes.jl:220\n\n\n\n\n\n\nModel\n\n@model function model_lognormal(rt; min_rt=minimum(df.RT), condition=nothing)\n\n    # Priors \n    σ ~ truncated(Normal(0, 0.5); lower=0)\n    τ ~ truncated(Gamma(1.1, 11); upper=min_rt)\n\n    μ_intercept ~ Normal(0, 2)\n    μ_condition ~ Normal(0, 0.5)\n\n    for i in 1:length(rt)\n        μ = μ_intercept + μ_condition * condition[i]\n        rt[i] ~ ShiftedLogNormal(μ, σ, τ)\n    end\nend\n\nmodel = model_lognormal(df.RT; condition=df.Accuracy)\nchain_lognormal = sample(model, NUTS(), 400)\n\n# Summary (95% CI)\nquantile(chain_lognormal; q=[0.025, 0.975])\n\n┌ Info: Found initial step size\n└   ϵ = 0.00625\nSampling:  10%|█████                                    |  ETA: 0:00:04Sampling:  13%|██████                                   |  ETA: 0:00:05Sampling:  14%|██████                                   |  ETA: 0:00:05Sampling:  16%|███████                                  |  ETA: 0:00:06Sampling:  17%|███████                                  |  ETA: 0:00:06Sampling:  22%|██████████                               |  ETA: 0:00:05Sampling:  28%|████████████                             |  ETA: 0:00:04Sampling:  32%|██████████████                           |  ETA: 0:00:03Sampling:  37%|████████████████                         |  ETA: 0:00:03Sampling:  42%|██████████████████                       |  ETA: 0:00:03Sampling:  48%|████████████████████                     |  ETA: 0:00:02Sampling:  54%|███████████████████████                  |  ETA: 0:00:02Sampling:  60%|█████████████████████████                |  ETA: 0:00:01Sampling:  66%|████████████████████████████             |  ETA: 0:00:01Sampling:  72%|██████████████████████████████           |  ETA: 0:00:01Sampling:  79%|█████████████████████████████████        |  ETA: 0:00:01Sampling:  86%|████████████████████████████████████     |  ETA: 0:00:00Sampling:  92%|██████████████████████████████████████   |  ETA: 0:00:00Sampling:  98%|████████████████████████████████████████ |  ETA: 0:00:00Sampling: 100%|█████████████████████████████████████████| Time: 0:00:02\n\n\n\nQuantiles\n   parameters      2.5%     97.5% \n       Symbol   Float64   Float64 \n            σ    0.3333    0.3449\n            τ    0.1727    0.1795\n  μ_intercept   -1.1610   -1.1323\n  μ_condition    0.3167    0.3437\n\n\n\n\n\n\nCode\npred = predict(model_lognormal([(missing) for i in 1:length(df.RT)]; condition=df.Accuracy), chain_lognormal)\npred = Array(pred)\n\n\n\n\nCode\nfig = plot_distribution(df, \"Predictions made by Shifted LogNormal Model\")\nfor i in 1:length(chain_lognormal)\n    lines!(Makie.KernelDensity.kde(pred[:, i]), color=ifelse(df.Accuracy[i] == 1, \"#388E3C\", \"#D32F2F\"), alpha=0.1)\nend\nfig\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie C:\\Users\\domma\\.julia\\packages\\Makie\\VRavR\\src\\scenes.jl:220\n\n\n\n\n\n\n\nMore Conditional Parameters\n\n\nCode\nxaxis = range(-6, 2, 1000)\nfig = Figure()\nax = Axis(fig[1, 1])\nlines!(xaxis, pdf.(-Weibull(2, 2.5)+1, xaxis); color=\"red\")\nlines!(xaxis, pdf.(-Weibull(2.5, 2)+1, xaxis); color=\"orange\")\nlines!(xaxis, pdf.(-Weibull(2.5, 2.5)+1, xaxis); color=\"blue\")\nlines!(xaxis, pdf.(-Weibull(2.5, 3)+1, xaxis); color=\"green\")\nax2 = Axis(fig[1, 2])\nlines!(exp.(xaxis), pdf.(-Weibull(2, 2.5)+1, xaxis); color=\"red\")\nlines!(exp.(xaxis), pdf.(-Weibull(2.5, 2)+1, xaxis); color=\"orange\")\nlines!(exp.(xaxis), pdf.(-Weibull(2.5, 2.5)+1, xaxis); color=\"blue\")\nlines!(exp.(xaxis), pdf.(-Weibull(2.5, 3)+1, xaxis); color=\"green\")\nfig\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie C:\\Users\\domma\\.julia\\packages\\Makie\\VRavR\\src\\scenes.jl:220\n\n\n\n\n\n\n@model function model_lognormal2(rt; min_rt=minimum(df.RT), condition=nothing)\n\n    # Priors \n    τ ~ truncated(Gamma(1.1, 11); upper=min_rt)\n\n    μ_intercept ~ Normal(0, 2)\n    μ_condition ~ Normal(0, 0.5)\n\n    σ_intercept ~ -Weibull(2.5, 3) + 1\n    σ_condition ~ Normal(0, 0.01)\n\n    for i in 1:length(rt)\n        μ = μ_intercept + μ_condition * condition[i]\n        σ = σ_intercept + σ_condition * condition[i]\n        rt[i] ~ ShiftedLogNormal(μ, exp(σ), τ)\n    end\nend\n\nmodel = model_lognormal2(df.RT; condition=df.Accuracy)\nchain_lognormal2 = sample(model, NUTS(), 400)\n\n# Summary (95% CI)\nquantile(chain_lognormal2; q=[0.025, 0.975])\n\n┌ Info: Found initial step size\n└   ϵ = 0.0015625\nSampling:   7%|███                                      |  ETA: 0:00:03Sampling:  10%|█████                                    |  ETA: 0:00:03Sampling:  14%|██████                                   |  ETA: 0:00:04Sampling:  17%|███████                                  |  ETA: 0:00:04Sampling:  20%|████████                                 |  ETA: 0:00:04Sampling:  23%|██████████                               |  ETA: 0:00:04Sampling:  26%|███████████                              |  ETA: 0:00:04Sampling:  30%|█████████████                            |  ETA: 0:00:03Sampling:  32%|██████████████                           |  ETA: 0:00:03Sampling:  36%|███████████████                          |  ETA: 0:00:03Sampling:  39%|████████████████                         |  ETA: 0:00:03Sampling:  43%|██████████████████                       |  ETA: 0:00:02Sampling:  46%|████████████████████                     |  ETA: 0:00:02Sampling:  50%|█████████████████████                    |  ETA: 0:00:02Sampling:  53%|██████████████████████                   |  ETA: 0:00:02Sampling:  56%|████████████████████████                 |  ETA: 0:00:02Sampling:  60%|█████████████████████████                |  ETA: 0:00:02Sampling:  63%|██████████████████████████               |  ETA: 0:00:02Sampling:  66%|████████████████████████████             |  ETA: 0:00:01Sampling:  70%|█████████████████████████████            |  ETA: 0:00:01Sampling:  74%|███████████████████████████████          |  ETA: 0:00:01Sampling:  78%|█████████████████████████████████        |  ETA: 0:00:01Sampling:  82%|██████████████████████████████████       |  ETA: 0:00:01Sampling:  86%|████████████████████████████████████     |  ETA: 0:00:01Sampling:  89%|█████████████████████████████████████    |  ETA: 0:00:00Sampling:  93%|███████████████████████████████████████  |  ETA: 0:00:00Sampling:  96%|████████████████████████████████████████ |  ETA: 0:00:00Sampling:  99%|█████████████████████████████████████████|  ETA: 0:00:00Sampling: 100%|█████████████████████████████████████████| Time: 0:00:03\n\n\n\nQuantiles\n   parameters      2.5%     97.5% \n       Symbol   Float64   Float64 \n            τ    0.1727    0.1794\n  μ_intercept   -1.1602   -1.1300\n  μ_condition    0.3149    0.3429\n  σ_intercept   -1.1247   -1.0890\n  σ_condition    0.0259    0.0559\n\n\n\n\n\n\nCode\npred = predict(model_lognormal2([(missing) for i in 1:length(df.RT)]; condition=df.Accuracy), chain_lognormal2)\npred = Array(pred)\n\n\n\n\nCode\nfig = plot_distribution(df, \"Predictions made by Shifted LogNormal Model\")\nfor i in 1:length(chain_lognormal2)\n    lines!(Makie.KernelDensity.kde(pred[:, i]), color=ifelse(df.Accuracy[i] == 1, \"#388E3C\", \"#D32F2F\"), alpha=0.1)\nend\nfig\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie C:\\Users\\domma\\.julia\\packages\\Makie\\VRavR\\src\\scenes.jl:220\n\n\n\n\n\n\n\n\n4.2.5 ExGaussian Model\n\nEx-Gaussian models in R: A Tutorial\n\n\n\n4.2.6 Wald Model\nMoe from statistical models that describe to models that generate RT-like data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reaction Times</span>"
    ]
  },
  {
    "objectID": "4_rt.html#generative-models-ddm",
    "href": "4_rt.html#generative-models-ddm",
    "title": "4  Reaction Times",
    "section": "4.3 Generative Models (DDM)",
    "text": "4.3 Generative Models (DDM)\nUse DDM as a case study to introduce generative models\n\nDrift Diffusion Model (DDM) in R: A Tutorial",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reaction Times</span>"
    ]
  },
  {
    "objectID": "4_rt.html#other-models-lba-lnr",
    "href": "4_rt.html#other-models-lba-lnr",
    "title": "4  Reaction Times",
    "section": "4.4 Other Models (LBA, LNR)",
    "text": "4.4 Other Models (LBA, LNR)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reaction Times</span>"
    ]
  },
  {
    "objectID": "4_rt.html#including-random-effects",
    "href": "4_rt.html#including-random-effects",
    "title": "4  Reaction Times",
    "section": "4.5 Including Random Effects",
    "text": "4.5 Including Random Effects\nTODO.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reaction Times</span>"
    ]
  },
  {
    "objectID": "4_rt.html#additional-resources",
    "href": "4_rt.html#additional-resources",
    "title": "4  Reaction Times",
    "section": "4.6 Additional Resources",
    "text": "4.6 Additional Resources\n\nLindelov’s overview of RT models: An absolute must-read.\nDe Boeck & Jeon (2019): A paper providing an overview of RT models.\nhttps://github.com/vasishth/bayescogsci\n\n\n\n\n\nHeathcote, Andrew, and Jonathon Love. 2012. “Linear Deterministic Accumulator Models of Simple Choice.” Frontiers in Psychology 3: 292.\n\n\nLo, Steson, and Sally Andrews. 2015. “To Transform or Not to Transform: Using Generalized Linear Mixed Models to Analyse Reaction Time Data.” Frontiers in Psychology 6: 1171.\n\n\nSchramm, Pele, and Jeffrey N Rouder. 2019. “Are Reaction Time Transformations Really Beneficial?”\n\n\nThériault, Rémi, Mattan S Ben-Shachar, Indrajeet Patil, Daniel Lüdecke, Brenton M Wiernik, and Dominique Makowski. 2024. “Check Your Outliers! An Introduction to Identifying Statistical Outliers in r with Easystats.” Behavior Research Methods 56 (4): 4162–72.\n\n\nWagenmakers, Eric-Jan, Roger Ratcliff, Pablo Gomez, and Gail McKoon. 2008. “A Diffusion Model Account of Criterion Shifts in the Lexical Decision Task.” Journal of Memory and Language 58 (1): 140–59.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reaction Times</span>"
    ]
  },
  {
    "objectID": "5_individual.html",
    "href": "5_individual.html",
    "title": "5  Individual Parameters",
    "section": "",
    "text": "From mixed models\nAs prior-informed individual Bayesian models",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Individual Parameters</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Heathcote, Andrew, and Jonathon Love. 2012. “Linear Deterministic\nAccumulator Models of Simple Choice.” Frontiers in\nPsychology 3: 292.\n\n\nLo, Steson, and Sally Andrews. 2015. “To Transform or Not to\nTransform: Using Generalized Linear Mixed Models to Analyse Reaction\nTime Data.” Frontiers in Psychology 6: 1171.\n\n\nSchramm, Pele, and Jeffrey N Rouder. 2019. “Are Reaction Time\nTransformations Really Beneficial?”\n\n\nThériault, Rémi, Mattan S Ben-Shachar, Indrajeet Patil, Daniel Lüdecke,\nBrenton M Wiernik, and Dominique Makowski. 2024. “Check Your\nOutliers! An Introduction to Identifying Statistical Outliers in r with\nEasystats.” Behavior Research Methods 56 (4): 4162–72.\n\n\nWagenmakers, Eric-Jan, Roger Ratcliff, Pablo Gomez, and Gail McKoon.\n2008. “A Diffusion Model Account of Criterion Shifts in the\nLexical Decision Task.” Journal of Memory and Language\n58 (1): 140–59.",
    "crumbs": [
      "References"
    ]
  }
]