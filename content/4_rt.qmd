# Reaction Times

![](https://img.shields.io/badge/status-WIP-orange)

## The Data

Data from @wagenmakers2008diffusion - Experiment 1.
Using the same procedure as the authors, we excluded all trials with uninterpretable response time [see @theriault2024check], i.e., responses that are too fast response (<180 ms) or too slow (>2 sec instead of >3 sec).

```{julia}
#| code-fold: false

using Downloads, CSV, DataFrames
using Turing, Distributions, SequentialSamplingModels
using GLMakie

df = CSV.read(Downloads.download("https://raw.githubusercontent.com/DominiqueMakowski/CognitiveModels/main/data/wagenmakers2008.csv"), DataFrame)

# Show 10 first rows
first(df, 10)
```

In the previous chapter, we modelled the error rate (the probability of making an error) using a logistic model, and observed that it was higher in the `"Speed"` condition. 
But how about speed? We are going to first take interest in the RT of **Correct** answers only (as we can assume that errors are underpinned by a different *generative process*). 

After filtering out the errors, we create a new column, `Accuracy`, which is the "binarization" of the `Condition` column, and is equal to 1 when the condition is `"Accuracy"` and 0 when it is `"Speed"`.

```{julia}
#| output: false

df = df[df.Error .== 0, :]
df.Accuracy = df.Condition .== "Accuracy"
```


::: {.callout-tip title="Code Tip"}
Note the usage of *vectorization* `.==` as we want to compare each element of the `Condition` vector to the target `"Accuracy"`.
:::

```{julia}
function plot_distribution(df, title="Empirical Distribution of Data from Wagenmakers et al. (2018)")
    fig = Figure()
    ax = Axis(fig[1, 1], title=title,
        xlabel="RT (s)",
        ylabel="Distribution",
        yticksvisible=false,
        xticksvisible=false,
        yticklabelsvisible=false)
    Makie.density!(df[df.Condition .== "Speed", :RT], color=("#EF5350", 0.7), label = "Speed")
    Makie.density!(df[df.Condition .== "Accuracy", :RT], color=("#66BB6A", 0.7), label = "Accuracy")
    Makie.axislegend("Condition"; position=:rt)
    Makie.ylims!(ax, (0, nothing))
    return fig
end

plot_distribution(df, "Empirical Distribution of Data from Wagenmakers et al. (2018)")
```


## Descriptive Models 

::: {.callout-note}
Note until the last section, we will disregard the existence of multiple participants (which require the inclusion of random effects in the model).
We will treat the data as if it was a single participant at first to better understand the parameters, but will show how to add random effects at the end.
:::

### Gaussian (aka *Linear*) Model

A linear model is the most common type of model. 
It aims at predicting the **mean** $\mu$ of the outcome variable using a **Normal** (aka *Gaussian*) distribution for the residuals.
In other words, it models the outcome $y$ as a Normal distribution with a mean $\mu$ that is itself hte result of a linear function of the predictors $X$ and a variance $\sigma$ that is constant across all values of the predictors.
It can be written as $y = Normal(\mu, \sigma)$, where $\mu = intercept + slope * X$.

In order to fit a Linear Model for RTs, we need to set a prior on all these parameters, namely:
- The variance $\sigma$ (correspondong to the "spread" of RTs)
- The mean $\mu$ for the intercept (i.e., at the reference condition which is in our case `"Speed"`)
- The effect of the condition (the slope).

#### Model Specification

```{julia}
#| code-fold: false
#| output: false

@model function model_Gaussian(rt; condition=nothing)

    # Set priors on variance, intercept and effect of condition
    σ ~ truncated(Normal(0, 1); lower=0)

    μ_intercept ~ truncated(Normal(0, 1); lower=0)
    μ_condition ~ Normal(0, 0.5)

    for i in 1:length(rt)
        μ = μ_intercept + μ_condition * condition[i]
        rt[i] ~ Normal(μ, σ)
    end
end


model = model_Gaussian(df.RT; condition=df.Accuracy)
chain_Gaussian = sample(model, NUTS(), 400)
```

```{julia}
#| code-fold: false

# Summary (95% CI)
hpd(chain_Gaussian; alpha=0.05)
```


The effect of Condition is significant, people are on average slower (higher RT) when condition is `"Accuracy"`.
But is our model good?

#### Posterior Predictive Check

```{julia}
#| output: false

pred = predict(model_Gaussian([(missing) for i in 1:length(df.RT)], condition=df.Accuracy), chain_Gaussian)
pred = Array(pred)
```

```{julia}
#| fig-width: 10
#| fig-height: 7

fig = plot_distribution(df, "Predictions made by Gaussian (aka Linear) Model")
for i in 1:length(chain_Gaussian)
    lines!(Makie.KernelDensity.kde(pred[:, i]), color=ifelse(df.Accuracy[i] == 1, "#388E3C", "#D32F2F"), alpha=0.1)
end
fig
```

### Scaled Gaussian Model

The previous model, despite its poor fit to the data, suggests that the mean RT is higher for the `Accuracy` condition. But it seems like the distribution is also *wider* (response time is more variable). 
Typical linear model estimate only one value for sigma $\sigma$ for the whole model, hence the requirement for **homoscedasticity**.

::: {.callout-note}
**Homoscedasticity**, or homogeneity of variances, is the assumption of similar variances accross different values of predictors. 
It is important in linear models as only one value for sigma $\sigma$ is estimated.
:::

Is it possible to set sigma $\sigma$ as a parameter that would depend on the condition, in the same way as mu $\mu$? In Julia, this is very simple.

All we need is to set sigma $\sigma$ as the result of a linear function, such as $\sigma = intercept + slope * condition$.
This means setting a prior on the intercept of sigma $\sigma$ (in our case, the variance in the reference condition) and a prior on how much this variance changes for the other condition.
This change can, by definition, be positive or negative (i.e., the other condition can have either a biggger or a smaller variance), so the prior over the effect of condition should ideally allow for positive and negative values (e.g., `σ_condition ~ Normal(0, 0.1)`).

But this leads to an **important problem**.

::: {.callout-important}
The combination of an intercept and a (possible negative) slope for sigma $\sigma$ technically allows for negative variance values, which is impossible (distributions cannot have a negative variance).
This issue is one of the most important to address when setting up complex models for RTs.
:::

Indeed, even if we set a very narrow prior on the intercept of sigma $\sigma$ to fix it at for instance **0.14**, and a narrow prior on the effect of condition, say $Normal(0, 0.001)$, an effect of condition of **-0.15** is still possible (albeit with very low probability). 
And such effect would lead to a sigma $\sigma$ of **0.14 - 0.15 = -0.01**, which would lead to an error (and this will often happen as the sampling process does explore unlikely regions of the parameter space).


#### Solution 1: Directional Effect of Condition

One possible (but not recommended) solution is to simply make it impossible for the effect of condition to be negative. 
This can work in our case, because we know that the comparison condition is likely to have a higher variance than the reference condition (the intercept) - and if it wasn't the case, we could have changed the reference factor.


```{julia}
#| code-fold: false
#| output: false

@model function model_ScaledlGaussian1(rt; condition=nothing)

    # Priors
    μ_intercept ~ truncated(Normal(0, 1); lower=0)
    μ_condition ~ Normal(0, 0.5)

    σ_intercept ~ truncated(Normal(0, 1); lower=0)  # Same prior as previously
    σ_condition ~ truncated(Normal(0, 0.1); lower=0)  # Enforce positivity

    for i in 1:length(rt)
        μ = μ_intercept + μ_condition * condition[i]
        σ = σ_intercept + σ_condition * condition[i]
        rt[i] ~ Normal(μ, σ)
    end
end

model = model_ScaledlGaussian1(df.RT; condition=df.Accuracy)
chain_ScaledGaussian = sample(model, NUTS(), 400)
```

```{julia}
#| code-fold: false

# Summary (95% CI)
hpd(chain_ScaledGaussian; alpha=0.05)
```

We can see that the effect of condition on sigma $\sigma$ is significantly positive: the variance is higher in the `Accuracy` condition as compared to the `Speed` condition. 

#### Solution 2: Avoid Exploring Negative Variance Values

The other trick is to force the sampling algorithm to avoid exploring negative variance values (when sigma $\sigma$ < 0).
This can be done by adding a conditional statement when sigma $\sigma$ is negative to avoid trying this value and erroring, and instead returning an infinitely low model probability (`-Inf`) to push away the exploration of this impossible region.

```{julia}
#| code-fold: false
#| output: false

@model function model_ScaledlGaussian(rt; condition=nothing)

    # Priors
    μ_intercept ~ truncated(Normal(0, 1); lower=0)
    μ_condition ~ Normal(0, 0.5)

    σ_intercept ~ truncated(Normal(0, 1); lower=0)
    σ_condition ~ Normal(0, 0.1)

    for i in 1:length(rt)
        μ = μ_intercept + μ_condition * condition[i]
        σ = σ_intercept + σ_condition * condition[i]
        if σ < 0  # Avoid negative variance values
            Turing.@addlogprob! -Inf
            return nothing
        end
        rt[i] ~ Normal(μ, σ)
    end
end

model = model_ScaledlGaussian(df.RT; condition=df.Accuracy)
chain_ScaledGaussian = sample(model, NUTS(), 400)
```

```{julia}
#| code-fold: false

hpd(chain_ScaledGaussian; alpha=0.05)
```

```{julia}
pred = predict(model_ScaledlGaussian([(missing) for i in 1:length(df.RT)], condition=df.Accuracy), chain_ScaledGaussian)
pred = Array(pred)

fig = plot_distribution(df, "Predictions made by Scaled Gaussian Model")
for i in 1:length(chain_ScaledGaussian)
    lines!(Makie.KernelDensity.kde(pred[:, i]), color=ifelse(df.Accuracy[i] == 1, "#388E3C", "#D32F2F"), alpha=0.1)
end
fig
```



<!-- #### Solution 3: Express Variance on the Exponential Scale
See https://github.com/itsdfish/SequentialSamplingModels.jl/issues/78#issuecomment-2211702253 
IS THAT RIGHT? -->

Although relaxing the homoscedasticity assumption is a good step forward, allowing us to make **richer conclusions** and better capturing the data.
Despite that, the Gaussian model stil seem to be a poor fit to the data.

### The Problem with Linear Models

Reaction time (RTs) have been traditionally modeled using traditional linear models and their derived statistical tests such as *t*-test and ANOVAs. Importantly, linear models - by definition - will try to predict the *mean* of the outcome variable by estimating the "best fitting" *Normal* distribution. In the context of reaction times (RTs), this is not ideal, as RTs typically exhibit a non-normal distribution, skewed towards the left with a long tail towards the right. This means that the parameters of a Normal distribution (mean $\mu$ and standard deviation $\sigma$) are not good descriptors of the data.

![](media/rt_normal.gif)

> Linear models try to find the best fitting Normal distribution for the data. However, for reaction times, even the best fitting Normal distribution (in red) does not capture well the actual data (in grey).

A popular mitigation method to account for the non-normality of RTs is to transform the data, using for instance the popular *log-transform*. 
However, this practice should be avoided as it leads to various issues, including loss of power and distorted results interpretation [@lo2015transform; @schramm2019reaction].
Instead, rather than applying arbitrary data transformation, it would be better to swap the Normal distribution used by the model for a more appropriate one that can better capture the characteristics of a RT distribution.


### Shifted LogNormal Model

One of the obvious candidate alternative to the log-transformation would be to use a model with a Log-transformed Normal distribution.

New parameter, $\tau$ (Tau for delay), which corresponds to the "starting time". We need to set a prior for this parameter, which is usually truncated between 0 and the minimum RT of the data (the logic being that the minimum delay for response must be lower than the faster response actually observed).

```{julia}
xaxis = range(0, 1, 1000)
lines(xaxis, pdf.(Gamma(1.1, 11), xaxis))
```

#### Model

```{julia}
#| code-fold: false

@model function model_lognormal(rt; min_rt=minimum(df.RT), condition=nothing)

    # Priors 
    σ ~ truncated(Normal(0, 0.5); lower=0)
    τ ~ truncated(Gamma(1.1, 11); upper=min_rt)

    μ_intercept ~ Normal(0, 2)
    μ_condition ~ Normal(0, 0.5)

    for i in 1:length(rt)
        μ = μ_intercept + μ_condition * condition[i]
        rt[i] ~ ShiftedLogNormal(μ, σ, τ)
    end
end

model = model_lognormal(df.RT; condition=df.Accuracy)
chain_lognormal = sample(model, NUTS(), 400)

# Summary (95% CI)
quantile(chain_lognormal; q=[0.025, 0.975])
```

```{julia}
#| output: false

pred = predict(model_lognormal([(missing) for i in 1:length(df.RT)]; condition=df.Accuracy), chain_lognormal)
pred = Array(pred)
```

```{julia}
#| fig-width: 10
#| fig-height: 7

fig = plot_distribution(df, "Predictions made by Shifted LogNormal Model")
for i in 1:length(chain_lognormal)
    lines!(Makie.KernelDensity.kde(pred[:, i]), color=ifelse(df.Accuracy[i] == 1, "#388E3C", "#D32F2F"), alpha=0.1)
end
fig
```

#### More Conditional Parameters

```{julia}
xaxis = range(-6, 2, 1000)
fig = Figure()
ax = Axis(fig[1, 1])
lines!(xaxis, pdf.(-Weibull(2, 2.5)+1, xaxis); color="red")
lines!(xaxis, pdf.(-Weibull(2.5, 2)+1, xaxis); color="orange")
lines!(xaxis, pdf.(-Weibull(2.5, 2.5)+1, xaxis); color="blue")
lines!(xaxis, pdf.(-Weibull(2.5, 3)+1, xaxis); color="green")
ax2 = Axis(fig[1, 2])
lines!(exp.(xaxis), pdf.(-Weibull(2, 2.5)+1, xaxis); color="red")
lines!(exp.(xaxis), pdf.(-Weibull(2.5, 2)+1, xaxis); color="orange")
lines!(exp.(xaxis), pdf.(-Weibull(2.5, 2.5)+1, xaxis); color="blue")
lines!(exp.(xaxis), pdf.(-Weibull(2.5, 3)+1, xaxis); color="green")
fig
```

```{julia}
#| code-fold: false

@model function model_lognormal2(rt; min_rt=minimum(df.RT), condition=nothing)

    # Priors 
    τ ~ truncated(Gamma(1.1, 11); upper=min_rt)

    μ_intercept ~ Normal(0, 2)
    μ_condition ~ Normal(0, 0.5)

    σ_intercept ~ -Weibull(2.5, 3) + 1
    σ_condition ~ Normal(0, 0.01)

    for i in 1:length(rt)
        μ = μ_intercept + μ_condition * condition[i]
        σ = σ_intercept + σ_condition * condition[i]
        rt[i] ~ ShiftedLogNormal(μ, exp(σ), τ)
    end
end

model = model_lognormal2(df.RT; condition=df.Accuracy)
chain_lognormal2 = sample(model, NUTS(), 400)

# Summary (95% CI)
quantile(chain_lognormal2; q=[0.025, 0.975])
```

```{julia}
#| output: false

pred = predict(model_lognormal2([(missing) for i in 1:length(df.RT)]; condition=df.Accuracy), chain_lognormal2)
pred = Array(pred)
```

```{julia}
#| fig-width: 10
#| fig-height: 7

fig = plot_distribution(df, "Predictions made by Shifted LogNormal Model")
for i in 1:length(chain_lognormal2)
    lines!(Makie.KernelDensity.kde(pred[:, i]), color=ifelse(df.Accuracy[i] == 1, "#388E3C", "#D32F2F"), alpha=0.1)
end
fig
```


### ExGaussian Model


- [**Ex-Gaussian models in R: A Tutorial**](https://dominiquemakowski.github.io/easyRT/articles/exgaussian.html)

### Wald Model

Moe from statistical models that *describe* to models that *generate* RT-like data.

## Generative Models (DDM)

Use DDM as a case study to introduce generative models

- [**Drift Diffusion Model (DDM) in R: A Tutorial**](https://dominiquemakowski.github.io/easyRT/articles/ddm.html)

## Other Models (LBA, LNR)

## Including Random Effects

TODO.

## Additional Resources

- [**Lindelov's overview of RT models**](https://lindeloev.github.io/shiny-rt/): An absolute must-read.
- [**De Boeck & Jeon (2019)**](https://www.frontiersin.org/articles/10.3389/fpsyg.2019.00102/full): A paper providing an overview of RT models.
- [https://github.com/vasishth/bayescogsci](https://github.com/vasishth/bayescogsci)
