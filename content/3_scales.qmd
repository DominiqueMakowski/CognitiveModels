# Choices and Scales

![](https://img.shields.io/badge/status-not_started-red)


## Bounded Variables

While might be tempted to believe that most of the data that we collect in psychological science are true **continuous** variables, this is often not the case. 
In fact, many variables are **bounded**: there values are delimited by hard bounds. 
This is typically the case for slider (aka "analog" scakes), dimensions scores (average or sum) from multiple Likert scales, percentages, proportions, etc.

Most psychometric indices are bounded. 
For instance, the minimum and maximum values for the IQ test (WAIS-IV) are 45-155. It is 0 and 63 for the depression scale BDI-II, 20 and 80 for the STAI.

Despite this fact, we still most often use **linear models** to analyze these data, which is not ideal as it assumes that the dependent variable is continuous and normally distributed.

### The Problem with Linear Models

Let's take the data from @makowski2023novel that contains data from participants that underwent the Mini-IPIP6 personality test and the PID-5 BF questionnaire for "maladaptive" personality.
We will focus on the **"Disinhibition"** trait from the PID-5 BF questionnaire. 
Note that altough it is usually computed as the average of items a 4-point Likert scales **[0-3]**, this study used analog slides to obtain more finer-grained scores.

```{julia}
#| code-fold: false

using Downloads, CSV, DataFrames, Random
using Turing, Distributions, SequentialSamplingModels
using GLMakie

Random.seed!(123)  # For reproducibility

df = CSV.read(Downloads.download("https://raw.githubusercontent.com/DominiqueMakowski/CognitiveModels/main/data/makowski2023.csv"), DataFrame)

# Show 10 first rows
first(df, 10)

# Plot the distribution of "Disinhibition"
hist(df.Disinhibition, normalization = :pdf, color=:darkred, bins=40)
```


We will then fit a simple Gaussian model (an "intercept-only" linear model) that estimates the mean and the standard deviation of our variable of interest.


```{julia}
#| code-fold: false
#| output: false

@model function model_Gaussian(x)

    # Priors
    σ ~ truncated(Normal(0, 1); lower=0)  # Strictly positive half normal distribution
    μ ~ Normal(0, 3)

    # Iterate through every observation
    for i in 1:length(x)
        # Likelihood family
        x[i] ~ Normal(μ, σ)
    end
end

# Fit the model with the data
fit_Gaussian = model_Gaussian(df.Disinhibition)
# Sample results using MCMC
chain_Gaussian = sample(fit_Gaussian, NUTS(), 400)
```

Let see if the model managed to recover the mean and standard deviation of the data:

```{julia}
println("Mean of the data: $(round(mean(df.Disinhibition); digits=3)) vs. mean from the model: $(round(mean(chain_Gaussian[:μ]); digits=3))")
println("SD of the data: $(round(std(df.Disinhibition); digits=3)) vs. SD from the model: $(round(mean(chain_Gaussian[:σ]); digits=3))")
```

Impressive! The model managed to almost perfectly recover the mean and standard deviation of the data.
**That means we must have a good model, right?** Not so fast!

Linear models are *by definition* designed at recovering the mean of the outcome variables (and its SD, assuming it is inavriant across groups). That does not mean that they can **capture the full complexity of the data**.

Let us then jump straight into generating **predictions** from the model and plotting the results against the actual data to see how well the model fits the data (a procedure called the **posterior predictive check**).

```{julia}
#| output: false

pred = predict(model_Gaussian([(missing) for i in 1:length(df.Disinhibition)]), chain_Gaussian)
pred = Array(pred)
```

```{julia}
fig = hist(df.Disinhibition, normalization = :pdf, color=:darkred, bins=40)
for i in 1:length(chain_Gaussian)
    lines!(Makie.KernelDensity.kde(pred[i, :]), alpha=0.1, color=:black)
end
fig
```

As we can see, the model assumes that the data is normally distributed, in a way that allows for negative values and values above 3, which **are not possible** because our variable is, **by design**, bounded between 0 and 3 (at it is the result of the mean of variables between 0 and 3).
The linear model might thus not be the best choice for this type of data.

### Rescaling

Continuous variables can be trivially rescaled, which is often done to improve the interpretability of the results. 
For instance, a *z*-score is a rescaled variable with a mean of 0 and a standard deviation of 1.
Importantly, rescaling variables does not change the variable's distribution or the absolute relationship between variables. 
It does not alter the fundamental conclusions from an analysis, but can help with the interpretation of the results (or computational performance).

Two common rescalings are **standardization** (mean=0, SD=1) and **normalization** (range 0-1).
The benefits of standardization are the interpretation in terms of deviation (which can be compared across variables).
The benefits of normalization are the interpretation in terms of **"proportion"** (percentage): a value of $0.5$ (i.e., $50\%$) means that the value is in the middle of the range.
The latter is particularly useful for bounded variables, as it redefines their bounds to 0 and 1 and allows for a more intuitive interpretation (e.g., "Condition B led to an increase of 20% of the rating on that scale").

Let's rescale the "Disinhibition" variable to a 0-1 range:

```{julia}
#| code-fold: false

function data_rescale(x; old_range=[minimum(x), maximum(x)], new_range=[0, 1])
    return (x .- old_range[1]) ./ (old_range[2] - old_range[1]) .* (new_range[2] - new_range[1]) .+ new_range[1]
end

# Rescale the variable
df.Disinhibition2 = data_rescale(df.Disinhibition; old_range=[0, 3], new_range=[0, 1])
```



```{julia}
# Visualize
fig = hist(df.Disinhibition2, normalization = :pdf, color=:darkred, bins=40)
xlims!(-0.1, 1.1)
fig
```

### Beta Models

```{julia}
#| code-fold: false

function BetaMean(μ, σ)
    if σ <= 0 || σ >= sqrt(μ * (1 - μ))
        error("Standard deviation σ must be in the interval (0, sqrt(μ*(1-μ))=$(sqrt(μ*(1-μ)))).")
    end
    ν = μ * (1 - μ) / σ^2 - 1
    α = μ * ν
    β = (1 - μ) * ν

    return Beta(α, β)
end
```

::: {.callout-caution}
TODO: Replace if it is supported somewhere (e.g., [Distributions.jl](https://github.com/JuliaStats/Distributions.jl/issues/1877))
:::


![](media/scales_BetaMean.gif)

```{julia}
#| eval: false

# @model function model_Beta(x)
#     μ ~ Beta(1, 1)
#     σ ~ Uniform(eps(typeof(μ)), μ * (1 - μ) - eps(typeof(μ)))
#     for i in 1:length(x)
#         x[i] ~ MeanVarBeta(μ, σ)
#     end
# end
# chains = sample(model_Beta(rand(MeanVarBeta(0.5, 0.2), 200)), NUTS(), 500; 
#    initial_params=[0.5, 0.1])
```



###  OrdBeta Models

Need **help** to implement this in Turing!

- https://cran.r-project.org/web/packages/ordbetareg/vignettes/package_introduction.html
- https://stats.andrewheiss.com/compassionate-clam/notebook/ordbeta.html#ordered-beta-regression
- https://www.robertkubinec.com/post/limited_dvs/


## Logistic models for Binary Data

![](https://img.shields.io/badge/status-good_for_contributing-blue)


Use the speed accuracy data that we use in the next chapter.