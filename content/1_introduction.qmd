# Fundamentals of Bayesian Modeling in Julia

![](https://img.shields.io/badge/status-not_started-red)


## Very quick intro to Julia and Turing

Goal is to teach just enough so that the reader understands the code.

::: {.callout-important}

### Notable Differences with Python and R

These are the most common sources of confusion and errors for newcomers to Julia:

- **1-indexing**: Similarly to R, Julia uses 1-based indexing, which means that the first element of a vector is `x[1]` (not `x[0]` as in Python).
- **Positional; Keyword arguments**: Julia functions makes a clear distinction between positional and keyword arguments, and both are often separated by `;`. Positional arguments are typically passed without a name, while keyword arguments must be named (e.g., `scatter(0, 0; color=:red)`). Some functions might look like `somefunction(; arg1=val1, arg2=val2)`.
- **Symbols**: Some arguments are prefixed with `:` (e.g., `:red` in `scatter(0, 0; color=:red)`). These **symbols** are like character strings that are not manipulable (there are more efficient).
- **Explicit vectorization**: Julia does not vectorize operations by default. You need to use a dot `.` in front of functions and operators to have it apply element by element. For example, `sin.([0, 1, 2])` will apply the `sin()` function to each element of its vector.
- **In-place operations**: Julia has a strong emphasis on performance, and in-place operations are often used to avoid unnecessary memory allocations. When functions modify their input "in-place" (without returns), a band `!` is used. For example, assuming `x = [0]` (1-element vector containing 0), `push!(x, 2)` will modify `x` in place (it is equivalent to `x = push(x, 2)`).
:::


### Generate Data from Normal Distribution

```{julia}
#| output: false
using Turing, Distributions, Random
using Makie

# Random sample from a Normal(μ=100, σ=15)
iq = rand(Normal(100, 15), 500)
```

```{julia}
fig = Figure()
ax = Axis(fig[1, 1], title="Distribution")
density!(ax, iq)
fig
```

### Recover Distribution Parameters with Turing

```{julia}
@model function model_gaussian(x)
    # Priors
    μ ~ Uniform(0, 200)
    σ ~ Uniform(0, 30)

    # Check against each datapoint
    for i in 1:length(x)
        x[i] ~ Normal(μ, σ)
    end
end

model = model_gaussian(iq)
sampling_results = sample(model, NUTS(), 400)

# Summary (95% CI)
summarystats(sampling_results)
```


## Linear Models

Understand what the parameters mean (intercept, slopes, sigma).

## Boostrapping

Introduce concepts related to pseudo-posterior distribution description

## Hierarchical Models

Simpson's paradox, random effects, how to leverage them to model interindividual differences

## Bayesian estimation

introduce Bayesian estimation and priors over parameters

## Bayesian mixed linear regression

put everything together
