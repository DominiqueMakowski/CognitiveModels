# Fundamentals of Bayesian Modeling in Julia

![](https://img.shields.io/badge/status-not_started-red)


## Brief Intro to Julia and Turing

![](https://img.shields.io/badge/status-good_for_contributing-blue)


Goal is to teach just enough so that the reader understands the code. 
We won't be discussing things like plotting (as it highly depends on the package used).

::: {.callout-tip title="To go further"}

- [**Modern Julia Workflows**](https://modernjuliaworkflows.github.io/): Julia tutorial that takes you from zero to hero.

:::




### Installing Julia and Packages

TODO.


### Julia Basics

::: {.callout-important}

### Notable Differences with Python and R

These are the most common sources of confusion and errors for newcomers to Julia:

- **1-indexing**: Similarly to R, Julia uses 1-based indexing, which means that the first element of a vector is `x[1]` (not `x[0]` as in Python).
- **Positional; Keyword arguments**: Julia functions makes a clear distinction between positional and keyword arguments, and both are often separated by `;`. Positional arguments are typically passed without a name, while keyword arguments must be named (e.g., `scatter(0, 0; color=:red)`). Some functions might look like `somefunction(; arg1=val1, arg2=val2)`.
- **Symbols**: Some arguments are prefixed with `:` (e.g., `:red` in `scatter(0, 0; color=:red)`). These *symbols* are like character strings that are not manipulable (there are more efficient).
- **Explicit vectorization**: Julia does not vectorize operations by default. You need to use a dot `.` in front of functions and operators to have it apply element by element. For example, `sin.([0, 1, 2])` will apply the `sin()` function to each element of its vector.
- **In-place operations**: Julia has a strong emphasis on performance, and in-place operations are often used to avoid unnecessary memory allocations. When functions modify their input "in-place" (without returns), a band `!` is used. For example, assuming `x = [0]` (1-element vector containing 0), `push!(x, 2)` will modify `x` in place (it is equivalent to `x = push(x, 2)`).
- **Macros**: Some functions start with `@`. These are called macros and are used to manipulate the code before it is run. For example, `@time` will measure the time it takes to run the code that follows.
- **Unicode**: Julia is a modern language to supports unicode characters, which are used a lot for mathematical operations. You can get the *mu* `μ` character by typing `\mu` and pressing `TAB`.
:::


### Generate Data from Normal Distribution

```{julia}
#| output: false
#| code-fold: false

using Turing, Distributions, Random
using Makie

# Random sample from a Normal(μ=100, σ=15)
iq = rand(Normal(100, 15), 500)
```

```{julia}
fig = Figure()
ax = Axis(fig[1, 1], title="Distribution")
density!(ax, iq)
fig
```

### Recover Distribution Parameters with Turing

```{julia}
#| output: false
#| code-fold: false

@model function model_gaussian(x)
    # Priors
    μ ~ Uniform(0, 200)
    σ ~ Uniform(0, 30)

    # Check against each datapoint
    for i in 1:length(x)
        x[i] ~ Normal(μ, σ)
    end
end

fit_gaussian = model_gaussian(iq)
chain_gaussian = sample(fit_gaussian, NUTS(), 400)
```

Inspecting the chain variable will show various posterior statistics (including the mean, standard deviation, and diagnostic indices).

```{julia}
#| code-fold: false

chain_gaussian
```

For the purpose of this book, we will mostly focus on the 95% Credible Interval (CI), and we will assume that a parameter is ***"significant"*** if its CI does not include 0.

```{julia}
#| code-fold: false

# Summary (95% CI)
hpd(chain_gaussian)
```

## Bayesian Linear Models

![](https://img.shields.io/badge/status-good_for_contributing-blue)

Understand what the parameters mean (intercept, slopes, sigma): needs a nice graph (animation?) to illustrate that.
Simple linear regression in Turing.
Introduce Bayesian estimation and priors over parameters 


## Hierarchical Models

![](https://img.shields.io/badge/status-good_for_contributing-blue)

Simpson's paradox, random effects

These models can be leveraged to obtain individual indices useful to study interindividual differences (see last chapter).

